<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Computational Psychology — Unit 11</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<!-- <meta name="copyright" content="Copyright &169; 2010-2019 Shimon Edelman"/> -->
<meta name="font-size-adjustment" content="-1" /> <!-- DEFAULT SIZE -->
<link rel="stylesheet" href="../Slidy/w3c-blue3.css"
 type="text/css" media="screen, projection, print" />
 <link rel="stylesheet" href="extras.css"
 type="text/css" media="screen, projection, print" />
<script src="../Slidy/slidy.js" type="text/javascript">
</script>
</head>
<body>

<!-- 
<rdf:RDF xmlns="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<License rdf:about="http://creativecommons.org/licenses/by-sa/2.5/">
<permits rdf:resource="http://creativecommons.org/ns#Reproduction"/>
<permits rdf:resource="http://creativecommons.org/ns#Distribution"/>
<requires rdf:resource="http://creativecommons.org/ns#Notice"/>
<requires rdf:resource="http://creativecommons.org/ns#Attribution"/>
<permits rdf:resource="http://creativecommons.org/ns#DerivativeWorks"/>
<requires rdf:resource="http://creativecommons.org/ns#ShareAlike"/>
</License>
</rdf:RDF>
-->

<!-- this defines the slide background -->

<div class="background">
  <div class="header">
  <!-- sized and colored via CSS -->
  </div>
  <!-- hidden style graphics to ensure they are saved with other content -->
  <img class="hidden" src="../Slidy/bullet.png" alt="" />
  <img class="hidden" src="../Slidy/fold.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold.bmp" alt="" />
  <img class="hidden" src="../Slidy/fold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/nofold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-nofold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold-dim.gif" alt="" />

  <div class="footer">
  <!-- modify the following text as appropriate -->
  Unit 11 &#151;
  </div>
</div>

<!-- COVER PAGE SLIDE -->
<div class="slide cover">
  <div class="header">
    <h1>Computational Psychology</h1>
    <p><a href="http://kybele.psych.cornell.edu/~edelman">Shimon Edelman</a>,
    &lt;<a href="mailto:se37@cornell.edu">se37@cornell.edu</a>&gt;</p>
  </div>
  <div style="float:left">
    <h2>Unit 11: Language II</h2>
  </div>
  <img src="../Lake-Michigan-horizon.jpg" title="Computing the Mind"
  class="figure-right"  height=70%>

</div>
<!-- END COVER PAGE -->



      
<DIV CLASS="slide">
  <h1>recap: the nature of language; on to learning</h1>

  <P>
  <BR>
  <ul>
    <li>(I) what language is for:
      <BR>influencing/manipulation of others and self
      </li>
    <li>(II) what language is like:
      <BR>structured, situated, incremental, dynamically constrained,
	concurrent, multimodal social behavior
      </li>
    <li>(III) what it means to know language:
      <BR>to have skills for influencing behavior and thinking
      </li>
    <li><B>(IV) how language is learned and used</B>
    </li>
    </ul>

</Div>
  

    
<DIV CLASS="slide">
  <h1>(IV) how language is learned: "blooming, buzzing confusion"</h1>

  <img src="infant-surprised-face.jpg" class="figure-right">
  <P>
  <BR>
  "Experience, from the very first, presents us with concreted objects, vaguely
  continuous with the rest of the world which envelops them in space and time, and
  potentially divisible into inward elements and parts. [...] <B>The baby, assailed by
  eye, ear, nose, skin and entrails at once, feels it all as one great
  <SC>blooming, buzzing confusion</SC></B>[.] 
  <DIR>  <DIR> <DIR>
    [from <a href="http://psychclassics.asu.edu/James/Principles/index.htm"
    target=new><I>The Principles of Psychology</I></a> by <a 
    href="http://plato.stanford.edu/entries/james/" target=new>William
    James</a>, v.I, p.488 (1890)]
  </DIR>  </DIR>  </DIR>
  <P>

</div>




<DIV CLASS="slide">
  <h1>overcoming the confusion: STATISTICS to the rescue</h1>

  <P>
  <BR>
  "When only a small percentage of all possible sound-sequences actually
  occurs in utterances, one can identify the boundaries of words, and their
  relative likelihoods, from their sentential environment.
  <P>
  <B>It is an essential property of language that the combinations of words
  and utterances are not all equiprobable.</B>
  <P>
  It follows that whatever else there is to be said about the form of language,
  a fundamental task is to state the <SC>departures from equiprobability</SC> in
  sound- and word-sequences."
  <DIR><DIR><DIR>
    <a href="http://en.wikipedia.org/wiki/Zellig_Harris"
    target=new>Zellig S. Harris</a>, <I>A theory of language and information</I>
    (1991) 
  </DIR></DIR></DIR>

</div>




<DIV CLASS="slide">
  <h1>STATISTICS: the only way to discover structure in data</h1>

  <P>
  <BR>
  <img src="lang-ex-7-1.jpg">
  <HR>
  <P>
  Despite initial appearances, this sequence has some highly improbable
  properties. Can you tell what they are?
  <P>
  [And don't say "It's all Greek to me"]

</div>




<DIV CLASS="slide">
  <h1>structure from regularities</h1>

  <P>
  <BR>
  <img src="lang-ex-7-2.jpg">
  <HR>
  <P>
  Despite initial appearances, this sequence has some highly improbable
  properties. 
  <P>
  Specifically, it contains a <SC>repeating subsequence</SC> that is six
  symbols long. 

</div>




<DIV CLASS="slide">
  <h1>structure from regularities</h1>

  <img src="two-camouflaged-ptarmigan.jpg" class="figure-right" height=550>
  <P>
  <BR>
  The same computational problem arises in vision.
  <P>
  <HR>
  <P>
  Can you see the two <a href="http://en.wikipedia.org/wiki/Ptarmigan"
  target=new>ptarmigan</a> in this scene? 

</div>




<DIV CLASS="slide">
  <h1>learning language structure, step 1: going digital (discrete)</h1>

  <img src="spectrogram-0507.jpg" class="figure-right" height=375>
    <P>
      <BR>
  <ul>
    <li>
    Normal speech is continuous: there are no silences between words.
    </li>
    <li>
    A communication system that relies on a set of continuous, indivisible
    signals would be very, very difficult to learn and use [<font
 color=red>Why?</font> Cf. <a
 href="http://en.wikipedia.org/wiki/Jackson_Pollock" target=new>Pollock</a>!].
    </li>
    <li>
    This is probably why all human languages rely on combinatorially and
    hierarchically composed discrete signals.
    </li>
    <li>
    The smallest discrete (categorical) element of speech is the
    <a href="http://en.wikipedia.org/wiki/Phoneme"
    target=new>phoneme</a>.
    </li>
  </ul>
  <P>
  There are 14 phonemes in the utterance "they don't know where to go",
  whose <a
  href="http://en.wikipedia.org/wiki/Spectrogram" target=new>spectrogram</a>
  is shown here.
    
</div>
  


<DIV CLASS="slide">
  <h1>learning language structure, step 1: going digital (discrete)</h1>

  <P>
  <BR>
  <ul>
    <li>
    With discrete, categorical units, one can easily compute statistics
    &#151;  <SC>departures from equiprobability</SC> &#151;
    that make learning possible.
    </li>
    <li>
    Babies learn categorical representations of sound units, through
    exposure and socially guided learning, whose basis is <SC>alignment</SC>
    and <SC>comparison</SC> of snippets of speech. 
    <P>
    For example, a <SC>contrastive</SC> pair, such as
    <DIR>  <DIR>
      <B>BIG</B>  and <B>BAG</B>
    </DIR>  </DIR>
    <P>
    constitutes evidence that the content of the slot (<B>B<font color=red>__</font>G</B>) may serve as useful
    unit.
  </li>
</ul>

</div>




<DIV CLASS="slide">
  <h1>learning words from statistical regularities</h1>

  <img src="lang-prettybaby.gif" class="figure-right" >
  <P>
  <BR>
  One of the many relevant kinds of statistics is the <SC>transitional
  probability</SC> from one sound to next.
  <P>
  It is highest when the two sounds follow one another within a <SC>word</SC>. 
  Transitional probabilities straddling a word boundary will be
  relatively low.
  <P>

  For example, consider the sequence <B>prettybaby</B> in the context of a corpus in which
  <b>pretty</B> and <B>baby</b> appear paired with other words, and not just
  with each other: {<B>prettydoggie</B>, <B>prettybaby</B>,
  <B>nicedoggie</B>, <B>nicebaby</B>, <B>prettydoll</B>}.
  <P>
  Now, the probability of
  <DIR><DIR>
    <B>pret</B>-><B>ty</B>
  </DIR></DIR>
  <P>
  will be higher than the probability of
  <DIR><DIR>
    <B>ty</B>-><B>ba</B>
  </DIR></DIR>

</div>




<DIV CLASS="slide">
  <h1>babies learning to segment words from statistical regularities</h1>

  <video src="Saffran-language.mov" class="figure-right" controls> 
  </video>
  <P>
  <BR>
  The groundbreaking study of Saffran, Aslin, and Newport (1996) presented
  6-8 month old infants with 2-minute long snippets of synthesized
  speech in which the transitional probabilities were controlled.
  <P>
  A habituation / head-turn procedure was used to quantify the ability of the subjects
  to segment speech into "words".

</div>




<DIV CLASS="slide">
  <h1>babies learning to segment words from statistical regularities</h1>

  <img src="lang-stat-Saffran96.jpg" height=450 class="figure-right" >
  <P>
  <BR>
  Average <SC>listening times</SC> (and standard errors) for 8-month-old infants
  familiarized with a <a href="Saffran-exsound.mov" target=new>continuous sequence</a> of three-syllable items (novel
  "words") and then tested with two of these familiar items (shaded) and two
  items composed of parts of two words (open).

  <P>

  <B>The infants treated part-words as novel items and thus distinguished
  them from whole words</B>.

</div>


<!--

<DIV CLASS="slide">
  <h1>forward and backward probabilities</h1>

  <P>
  <BR>
  English:
  <DIR>
    go to school
  </DIR>
  <P>
  Korean: 
  <DIR>
    학교에 가자
  </DIR>

</DIV>

-->


<DIV CLASS="slide">
  <h1>learning words from statistical regularities in a stream of (digital) information</h1>

  <img src="lang-MEX-alice2.jpg" height=400 class="figure-right">
  <P>
  <BR>
  Two stages in the processing of <a href="http://www.sabian.org/alice.htm"
 target=new><I>Alice in Wonderland</I></a> by the <a
 href="../SolanHornRuppinEdelman-PNAS05.pdf" target=new><SC>adios</SC></a>
 algorithm, more about which later [Z. Solan,
  D. Horn, E. Ruppin and S. Edelman (2005). <I>Unsupervised Learning
 of Natural Languages</I>. Proc. Natl. Acad. Sci. 102:11629-11634.]

</div>




<DIV CLASS="slide">
  <h1>learning language structure, step 2: going recursive</h1>

  <P>
  <BR>
  To learn <SC>hierarchical</SC> structure   ("complexity from simplicity"),
  one must <SC>"go recursive"</SC> &#151; find some statistically
  significant units (constructions), add them to the lexicon/grammar, use them to look for
  higher-order units, etc.

  <DIR>
    Candidate structures found by <SC>aligning</SC> and <SC>comparing</SC> strings
    at a certain level of representation become units that participate
    in structure discovery at the next higher level (more about this in the
 next lecture).
  </DIR>
  <P>
  <HR>
  <table cellspacing=10 width=100%>
    <tr>
      <td align=center>
	the marmot in the hole
	<P>
	the big marmot
	<P>
	  the big brown marmot
	<P>
	  the marmot saw Trotsky
      </td>
      <td align=center>
	<img src="Minsky-marmot-Trotsky.jpg" height=250>
      </td>
    </tr>
    <!--
    <tr>
      <td align=center>
	<I>tail recursion</I>
      </td>
      <td align=center>
	<I>embedding recursion</I>
      </td>
    </tr>
    -->
  </table>

</div>




<DIV CLASS="slide">
  <h1>given some structure, we can attempt generalization</h1>

  <P>
  <BR>
  сталинкаменеваубил
  <P>
  сталинзиновьеваубил
  <P>
  сталинфрунзеубил

  <P>
  <HR>
  <P>
  To find structure, <sc>align</SC> and <SC>compare</SC> substrings.

</div>




<DIV CLASS="slide">
  <h1>structure and generalization</h1>

  <P>
  <BR>
  сталинкаменеваубил
  <P>
  сталинзиновьеваубил
  <P>
  сталинфрунзеубил
  <P>
  сталинвсехчьялояльностьбылаподмалейшимподозрением
  <BR>имногихчьялояльностьбылабезупречнойубил
  <P class="incremental">
  сталин<font color=red>всехчьялояльностьбылаподмалейшимподозрением
  <BR>имногихчьялояльностьбылабезупречной</font>убил
  </P>
  <HR>
  <P>
  To find structure, <sc>align</SC> and <SC>compare</SC> substrings.
  <P>
  Then, feel free to substitute equivalent items [note the <a
 href="http://en.wikipedia.org/wiki/Accusative" target=new>accusative</a> <a
 href="http://en.wikipedia.org/wiki/Grammatical_case" target=new>case</a>] in the proper context.

</div>


<!--

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: sentences of the first type</h1>

  <img src="Johnson65-list1.jpg">
  <P>
  <HR>
  <P>
  N. Johnson, <I>The psychological reality of phrase-structure rules</I>,
  J. of Verbal Learning and Verbal Behavior, 4:469-475 (1965).

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the hypothesized structure</h1>

  <img src="Johnson65-fig1a.jpg">

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: sentences of the second type</h1>

  <img src="Johnson65-list2.jpg">

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the hypothesized structure</h1>

  <img src="Johnson65-fig1b.jpg">  

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the results support the hypothesis</h1>

  <img src="Johnson65-fig1a.jpg">
  <img src="Johnson65-table2.jpg">
  <P>
  [The two-phrase results (first row in the transition error table) apply to this example]

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the results support the hypothesis</h1>

  <img src="Johnson65-fig1b.jpg">
  <img src="Johnson65-table2.jpg">
  <P>
  [The three-phrase results (second row in the transition error table) apply to this example]

</div>


  -->



<DIV CLASS="slide">
  <h1>must not forget: the SOCIAL nature of language learning</h1>

  <img src="GoldsteinEtAl10-fig1.png" class="figure-right" height=550>
  <P>
  <BR>
  Language learning / use is an embodied, situated, incremental, dynamically
  constrained, concurrent, multimodal social behavior.
  <P>
  [Michael H. Goldstein, Heidi R. Waterfall, Arnon Lotem, Joseph
    Y. Halpern, Jennifer A. Schwade, Luca Onnis, and Shimon Edelman
    (2010). <a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(10)00045-8" target=new><I>General
    cognitive principles for learning structure in time and
  space</I></a>, Trends in Cognitive Sciences 14:249-258.]

</div>


  


<DIV CLASS="slide">
  <h1>how "Motherese" facilitates alignment and comparison</h1>

  <img src="varsets-in-six-languages.png" class="figure-right">
  <P>
  <BR>
  Runs of partially matching (= alignable) sentences occur naturally in
  child-directed speech.
  <P>
  Naturalistic longitudinal studies (notably, by Heidi Waterfall) show that
  such <SC>variation sets</SC> facilitate mastery of structure by the child.
  <P>
  [The six examples here are all from the Child Language Data Exchange
  System, <a href="http://childes.psy.cmu.edu/"
  target=new>CHILDES</a>.] 

</div>
  



<DIV CLASS="slide">
  <h1>variation sets in word segmentation</h1>

  <P>
  <BR>

  Can the effectiveness of variation sets be demonstrated in a controlled
  expriment?
  <P>
  Onnis, Waterfall, and Edelman
  (<a href="https://www.semanticscholar.org/paper/Learn-locally%2C-act-globally%3A-Learning-language-from-Onnis-Waterfall/da9605a78b7ca1b5092d9cb5ed794aacb60f3010" target=new><I>Learn
  Locally, Act Globally: Learning
  Language from Variation Set Cues</I></a>, Cognition 109:423-430, 2008) had
  subjects listen to ~100 sentences generated by a small artificial
  grammar.
  <P>
  Half of the subjects heard these sentences arranged so that ~20%
  of them formed variation sets; the other half heard <I>exactly the same
  sentences</I> in a randomly scrambled order (no variation sets).
<PRE>
  ko si zu pa gu klo zi
  da pe ra pra ti
  ko si fa ma pju
  da zu pa pra ti
</PRE>

</div>
  


<DIV CLASS="slide">
  <h1>variation sets in word segmentation</h1>

  <P>
  <BR>
  <img src="Onnis-et-al-2008-fig2-cropped.png" class="figure-right">

  Can the effectiveness of variation sets be demonstrated in a controlled
  expriment?
  <P>
  Onnis, Waterfall, and Edelman (<a href="https://www.semanticscholar.org/paper/Learn-locally%2C-act-globally%3A-Learning-language-from-Onnis-Waterfall/da9605a78b7ca1b5092d9cb5ed794aacb60f3010" target=new><I>Learn Locally, Act Globally: Learning
  Language from Variation Set Cues</I></a>, Cognition 109:423-430, 2008) had
  subjects listen to ~100 sentences generated by a small artificial
  grammar.
  <P>
  Half of the subjects heard these sentences arranged so that ~20%
  of them formed variation sets; the other half heard <I>exactly the same
  sentences</I> in a randomly scrambled order (no variation sets).

  <P>
  "Words" that appeared in variation sets are more reliably segmented.
  </P>

</div>
  

<!--

<DIV CLASS="slide">
  <h1>variation sets in phrase segmentation</h1>

  <P>
  <BR>
  <img src="Onnis-et-al-2008-fig4-cropped.png" class="figure-right">

  Can the effectiveness of variation sets be demonstrated in a controlled
  expriment?
  <P>
  The second experiment focused on phrase segmentation instead of word
  segmentation.
  <P class="incremental">
  The outcome was the same: <B>variation sets facilitated
  segmentation</B>.
  </P>

</div>


<DIV CLASS="slide">
  <h1>NEXT: computational modeling of language learning (and use)</h1>

  <img src="taxonomy-of-models.png" height=450 class="figure-right">
  <P>
  <BR>
  A taxonomy of computational models of language acquisition (and use) —

  <P>
    [S. Edelman
    (2017). <a href="http://doi.org/10.1016/j.langsci.2017.04.003"
    target=new><I>Language and other complex behaviors: 
    unifying characteristics, computational models, neural mechanisms</I></a>,
    Language Sciences 62:91-123.]
  
</div>

-->  

    
<div class="footer">
<p>Last modified: Thu Aug 6 2020 at 10:10:37 EDT</p>
</div>
</body>
</html>
