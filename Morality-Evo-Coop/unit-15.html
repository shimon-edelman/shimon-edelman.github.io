<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Computing Societies — Unit 15</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<!-- <meta name="copyright" content="Copyright &169; 2020 Shimon Edelman"/> -->
<meta name="font-size-adjustment" content="-1" /> <!-- DEFAULT SIZE -->
<link rel="stylesheet" href="../Slidy/w3c-blue3.css"
 type="text/css" media="screen, projection, print" />
 <link rel="stylesheet" href="extras.css"
 type="text/css" media="screen, projection, print" />
<script src="../Slidy/slidy.js" type="text/javascript">
</script>
<script type="text/javascript"
  src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>

<!-- this defines the slide background -->

<div class="background">
  <div class="header">
  <!-- sized and colored via CSS -->
  </div>
  <!-- hidden style graphics to ensure they are saved with other content -->
  <img class="hidden" src="../Slidy/bullet.png" alt="" />
  <img class="hidden" src="../Slidy/fold.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold.bmp" alt="" />
  <img class="hidden" src="../Slidy/fold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/nofold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-nofold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold-dim.gif" alt="" />

  <div class="footer">
  <!-- modify the following text as appropriate -->
  Unit 15 &#151;
  </div>
</div>

<!-- COVER PAGE SLIDE -->
<div class="slide cover">
  <div class="header">
    <h1>Computing Societies</h1>
    <p><a href="http://kybele.psych.cornell.edu/~edelman">Shimon Edelman</a>,
    &lt;<a href="mailto:se37@cornell.edu">se37@cornell.edu</a>&gt;</p>
  </div>
  <div style="float:left">
    <h2>Unit 15: happiness (cont.)</h2>
  </div>
<!--  <img src="../Lake-Michigan-horizon.jpg" title="Computing the Mind"
  class="figure-right"  height=70%> -->

</div>
<!-- END COVER PAGE -->



<div  CLASS="slide">
  <h1>happiness (cont.)</h1>

  <P>
    <ul>
      <li>
	<I>Happiness as an intrinsic motivator in reinforcement
	  learning</I> [Gao and Edelman, 2016b].
      </li>
    </ul>
  <P>
    Reinforcement learning, a general and universally useful framework
    for learning from experience, has been broadly recognized as a
    critically important concept for understanding and shaping
    adaptive behavior, both in ethology and in artificial
    intelligence. A key component in reinforcement learning is the
    reward function, which, according to an emerging consensus, should
    be <B>intrinsic</B> to the learning agent and a matter of appraisal
    rather than a simple reflection of external outcomes.
  </P>
  <P>
    We describe an approach to intrinsically motivated reinforcement
    learning that involves various aspects of happiness,
    operationalized as dynamic estimates of well-being. In four
    experiments, in which simulated agents learned to explore and
    forage in simulated environments, we show that <I>agents whose
    reward function properly balances momentary (hedonic) and
    longer-term (eudaimonic) well-being outperform agents equipped
    with standard fitness-oriented reward functions</I>. Our findings
    suggest that happiness-based features can be useful in developing
    robust, general-purpose reward mechanisms for intrinsically
    motivated autonomous agents.
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>reinforcement learning</h1>

  <P>
    Reinforcement learning, which does take into account behavioral
    outcomes, yet does not require explicit instruction or error
    correction, is a universal computational framework for learning,
    in the sense that any learning problem can be cast as an instance
    of RL. In its reliance on occasionally delivered scalar rewards,
    RL mimics learning in humans and other animals. It is an effective
    method for learning under uncertainty in dynamic environments: the
    agent learns ‘‘on its own’’ with no examples of correct behavior
    being provided. It evaluates its actions in relation to
    observations and outcomes, through continued interaction with the
    environment
  </P>
  <P>
    The success of RL depends critically on the formulation of the
    reward function, which maps outcomes to reinforcement signals and
    thus constitutes a critical link in the chain of credit
    assignment.  It is possible to design reward functions by hand,
    but in complex environments it may be infeasible for the designer
    to handcraft an effective, let alone optimal, reward function.
  </P>
  <P>
    We propose and test a family of reward functions that involve a
    simple quantitative model of ‘‘happiness’’; an ongoing,
    dynamically maintained, multi-component self-estimate of
    well-being. <B>Our goal is to formulate a reward function that would
    be general enough to alleviate the need for handcrafting across
    domains, and also informative enough to be effective in specific
    tasks.</B>
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>reinforcement learning</h1>

  <P>
    Reinforcement learning is a general approach to sequential
    decision problems. By a process of trial-and-error, the agent
    faced with the decisions must learn a set of policies: a mapping
    that assigns observed states to actions. At each time step, the
    agent receives an observation \(s\) from its environment \(E\),
    takes an action \(a\), receives reward \(r\), and repeats this
    process until a time horizon. The history at time \(t\), denoted
    by \(h_t\), is the history of interaction
    \((s_1,a_1,r_1,\ldots,s_t, a_t, r_t)\). Under this formulation,
    the problem can be cast as a finite state Markov decision process
    (MDP) — a tuple \(M=(S, A, T, R, \gamma)\), where \(S\) is a
    finite set of states; \(A\) is a set of actions; \(T = P(s,s'|a),
    s \in S, s' \in S, a \in A\) with \(P(s,s'|a)\) is the probability
    of transitioning to state \(s'\) upon taking action \(a\) in state
    \(s\); \(R\) specifies the reward distribution; and \(\gamma\) is
    the temporal discount factor for future rewards.
  </P>
  <P>
    <B>The goal of the agent is to choose a policy [mapping states to
    actions] that will maximize its future cumulative rewards,
    typically the expected discounted sum over a infinite horizon.</B>
    Formally, this corresponds to maximizing

    \begin{equation}
    \sum_{t=0}^ \infty \gamma^t R_{a_t}(s_t,s_{t+1})
    \end{equation}

    where action \(a_t\) is chosen according to some optimal policy,
    \(R_{a_t}(s_t,s_{t+1})\) is the immediate expected reward received
    after taking action \(a_t\) from state \(s_t\) to state
    \(s_{t+1}\), and \(\gamma \in [0,1]\) is the discount factor.
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>choosing the reward function for intrinsically motivated reinforcement learning</h1>

  <P>
    <ul>
      <li>
	Let \(E\) denote some set of environments.
      </li>
      <li>
	Let \(h_t = {s_1 a_1, r_1, \ldots,s_t,a_t,r_t} \in H\) denote
	a particular history of interaction of the agent with the
	environment up to time-step \(t\), taken from a set \(H\) of
	possible finite histories. 
      </li>
      <li>
	The learning objectives are encoded in a fitness-based reward
	function, denoted by \(r^F : S \times A \times H \rightarrow
	\mathcal{R}\), which prescribes the agent's preferences during
	some history of interaction.
      </li>
      <li>
	Let \(R\) be some broader space of possible reward functions,
	which includes, but is not limited to, the fitness component
	\(r^F\).
      </li>
      <li>
	The agent's objective is defined by a primary reward function
	\(r : S \times A \times H \rightarrow \mathcal{R} \in R\). The
	agent's behavior under such a reward function takes into
	account the fitness objective, but is not constrained to
	follow it at all times. Rather, the agent acts so as to
	maximize the cumulative reward as defined by \(r\), which
	guides its behavior.
      </li>
      <li>
	Finally, \(\mathcal{F}(r)\) measures the impact of reward
	function \(r\) on the agent's fitness in the set of
	environments of interest \(E\).
      </li>
    </ul>
    <P>
      <B>The optimal reward problem (ORP) is an optimization task:
      choosing the best \(r^* \in R\). ORP maximizes the expected
      fitness or objective return with respect to a distribution over
      possible environments \(E\).</B> The optimal reward function
      \(r^*\) is defined as:

      \begin{equation}
      r^* = \textrm{argmax}_{r \in R} \mathcal{F}(r)
      \end{equation}
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>the use of emotions in RL</h1>

  <P>
    As in any search-based approach, the success of the above IMRL
    framework depends on whether or not the space \(R\) of reward
    functions that is being searched does in fact contain good
    candidate functions. Given the central role of emotions in
    regulating animal behavior, it makes sense to incorporate
    emotion-based components — in particular, happiness — into the
    reward functions that populate the space \(R\).
  </P>
  <P>
    In <a href="unit-14.html" target=u>previous work</a>, we
    investigated several possible formulations of happiness that
    involve hedonic and eudaimonic well-being, as well as their role
    in evolutionary success of agents in a variety of simulated
    environments.  We found that
    <ol type=i>
      <li>
	the effects of attaching more weight to eudaimonic (longer-term)
	than to momentary (hedonic) happiness and of extending the memory
	for past happiness are both stronger in an environment where food
	is scarce;
      </li>
      <li>
	‘‘relative consumption’’, in which the agent’s well-being is
	diminished by that of its neighbors, is more detrimental to
	survival when food is scarce;
      </li>
      <li>
	agents with a positive outlook, whose longer-term happiness
	gets increased more from positive events than decreased from
	negative ones, is generally advantageous.
      </li>
    </ol>
  <P>
    Here, we use those findings as a conceptual foundation for
    designing IMRL agents.
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>happiness-based IMRL: the hedonic contribution</h1>

  \(
  \newcommand\EUD{\mathcal{E}}
  \newcommand\HED{\mathcal{H}}
  \newcommand\Hs{\mathit{\mathcal{H}}_s}
  \newcommand\Ha{\mathit{\mathcal{H}}_a}
  \newcommand\Hn{\mathit{\mathcal{H}}_n}
  \newcommand\E{\mathit{E}}
  \newcommand\M{\mathit{M}}
  \newcommand\Hist{H}
  \)
  <P>
    We formulate the reward function \(r \in R\) as a linear
    combination of hedonic and eudaimonic well-being functions,
    \(\HED\) and \(\EUD\), each of which maps history \(h_t\) to a
    scalar value:

    \begin{equation}
    r(h_t) = \theta_{he} \HED(h_t) + (1-\theta_{he}) \EUD(h_t)
    \end{equation}

    where the hedonic/eudaimonic balance parameter \(\theta_{he}\)
    controls the relative contribution of the two.
  </P>
  <P>
    The hedonic well-being \(\HED\) consists of three factors: \(\Hs\)
    based on agent's current state \(s\); \(\Ha\) based on agent's
    action \(a\) and its current state \(s\); and \(\Hn\) based on
    affectively important outcomes achieved during the agent's
    interaction with the environment:

    \begin{equation}
    \HED =  \theta_s \Hs+ \theta_a \Ha + (1-\theta_s - \theta_a)\Hn
    \end{equation}
    
    The observation-based component is computed as follows:

    \begin{equation}
    \Hs^t(s_t,h_t)  =\left( \cfrac{1}{\lambda_s}\right) ^ {-c(s_t,h_t,\Delta t)}
    \end{equation}

    where \(\lambda_s\) is a positive constant in the range of
    \((0,1]\) and \(\Delta t\) is the duration of the agent's memory
    window. The novelty function \(c(s_t,h_t,\Delta t)\) tracks the
    number of time steps since the agent previously visited state
    \(s_t\) during history \(h_t\) in the past \(\Delta t\) time
    window. Here, we chose to only consider the recent history of
    duration \(\Delta t\) instead of the entire past history.

</DIV>



<div  CLASS="slide">
  <h1>happiness-based IMRL: the hedonic contribution (cont.)</h1>

  <P>
    The action decision-based hedonic component is computed as follows:

    \begin{equation}
    \Ha^t(a_t,s_t,h_t) =\left( \cfrac{1}{\lambda_a}\right) ^ {-c(a_t,s_t,h_t,\Delta t)}
    \end{equation}

    where the \(\lambda_a\) and \(\Delta t\) parameters are as above
    and the novelty function \(c(a_t,s_t,h_t,\Delta t)\) is the number
    of time steps since the agent previously executed action \(a_t\)
    while in state \(s_t\) during history \(h_t\) in the past \(\Delta
    t\) time window.

    The hedonic component associated with extrinsic outcomes is:

    \begin{equation}
    \Hn^t = n_t
    \end{equation}
    
    where \(n_t\) represents the affectively important outcomes
    obtained from the agent-environment interaction at time \(t\). In
    those of our experiments that involve foraging, we used \(n_t\) to
    represent the agent's encounters with food or poison.
  </P>

</DIV>



<div  CLASS="slide">
  <h1>happiness-based IMRL: the eudaimonic (long-term) contribution</h1>

  <P>
    The agent's eudaimonic well-being \(\EUD\) is then computed from
    its present value of \(\HED\), its memory of past values of
    \(\HED\) extending over a number of cycles, and the rates of rise
    and fall of \(\HED\). The present formulation of eudaimonic
    well-being is based on our previous work.

    \begin{equation}
    \EUD^{t} = (1-\theta_{p}-\theta_{n}) \left( \HED^{t} -
    \underbrace{\cfrac{1}{\Delta t} \sum_{i=t-\Delta t}^{t-1}
    \HED^{i}}_{\text{expectation of hedonic}} \right) +
    \sum_{i=t-\Delta t}^{t} \left(  s_{p}\left(\cfrac{d \HED^i}{di}
    \right) + s_{n}\left(\cfrac{d \HED^i}{di} \right)\right)
    \\
    s_p (x) = \left\{ \begin{array}{lc}
      \theta_p \cdot x & x\geq0 \\  
      0 & x < 0
	      \end{array}\right. \;\;\;\;\;\;
    s_n (x) = \left\{ \begin{array}{lc}
      0 & x \geq 0 \\  
      \theta_n \cdot x & x < 0
    \end{array}\right. \;\;
    \end{equation}
  </P>
  <P>
    where \(\Delta t\) is the extent of the memory window. The step
    function \(s_p\) selects the weight assigned to upswings of \(H\)
    and and \(s_n\) — to downswings; \(\theta_p\) and \(\theta_n\) are
    the respective weights. Thus, agents can in principle value
    positive and negative events differently.
  </P>
  <P>
    The intuition behind this formulation is that the agent may have a
    subjective expectation based on the past interaction with the
    environment. Its eudaimonic well-being \(\EUD\) is based on its
    current hedonic well-being and its expectation, as well as on its
    evaluation of the rise and fall of hedonic well-being. The three
    terms could be weighted differently.
  </P>

</DIV>



<div  CLASS="slide">
  <h1>the learning algorithm</h1>

  <img src="Gao16b-alg.png" class="figure-right" height=450>
  <P>
    The \(\epsilon\)-greedy
    <a href="https://en.wikipedia.org/wiki/Q-learning"
    target=new><B>Q-learning</B></a> algorithm chooses a random action
    (an <I>exploration</I> step) with probability \(0 \le \epsilon \le
    1\) and a <a href="https://en.wikipedia.org/wiki/Greedy_algorithm"
    target=new>greedy action</a> (an <I>exploitation</I> step, which
    maximizes the expected reward) with probability \(1 -
    \epsilon\). When \(\epsilon\) is set to \(0\), the choice is
    always greedy; the balance between exploration and exploitation in
    that case depends on the structure of the reward function.
  </P>
  <P>
    The algorithm is intended to be used in conjunction with a variety
    of reward functions, including notably intrinsic happiness-based
    ones, listed next. At each step, the agent selects its next action
    \(a_{t}\), given its past history \(h_t\). If the exploration
    parameter \(\epsilon\) is nonzero, there is a finite probability
    of taking a random action; otherwise the action \(a_t\) is chosen
    that maximizes the sum of the expected reward
    \(E\left(r(h_{t+1})\right)\) and the value associated with new
    state \(s_{t+1}\). After taking action \(a_t\) and extending the
    history \(h_{t+1}\), the agent updates its \(Q\)-table entry for
    state \(s_t\) and action \(a_t\), taking into account the
    difference between the actual reward \(r(h_{t+1})\) and expected
    reward \(E\left(r(h_{t+1})\right)\).
  </P>

</DIV>



<div  CLASS="slide">
  <h1>types of agents</h1>

  <P>
    <ul>
      <li>
	<B>Qh</B> — <I>\(Q\)-learning with hedonic reward.</I> This
	type of agent uses the greedy version (\(\epsilon = 0\)) of
	\(Q\)-learning with a hedonic-only happiness-based reward. To
	enforce hedonic-only reward, \(\theta_{he}\) is set to 1.
      </li>
      <li>
	<B>Qe</B> — <I>\(Q\)-learning with eudaimonic reward.</I> This
	type of agent uses the greedy version of \(Q\)-learning with
	an eudaimonic-only happiness-based reward.  To enforce
	eudaimonic-only reward, \(\theta_{he}\) is set to 0.
      </li>
      <li>
	<B>Qc</B> — <I>\(Q\)-learning with combined well-being
	reward.</I>  This type of agent uses the greedy version of
	\(Q\)-learning with a combined happiness-based reward. To
	enforce combined well-being reward that includes both hedonic
	and eudaimonic components, \(\theta_{he}\) is set to lie in
	the range of \([0.1,0.9]\).
      </li>
      <li>
	<B>Qwb</B> — <I>\(Q\)-learning with well-being reward.</I>
	This type of agent is a union of the above three types: it
	uses the greedy version of \(Q\)-learning with a
	happiness-based reward in which the parameter \(\theta_{he}\)
	takes values in the full range of \([0,1]\).
      </li>
      <li>
	<B>\(\epsilon\)Qwb</B> — <I>\(\epsilon\)-greedy \(Q\)-learning
	  with well-being reward.</I>  Like the previous type, this
	  agent uses the full range of happiness-based rewards, with
	  the parameter \(\theta_{he}\) taking values in
	  \([0,1]\). However, instead of greedy it uses the
	  \(\epsilon\)-greedy version of \(Q\)-learning.
      </li>
      <li>
	<B>\(\epsilon\)Qr</B> — <I>\(\epsilon\)-greedy \(Q\)-learning
	  with random reward.</I>  This type of agent, whose
	  performance serves as a simple baseline for comparisons,
	  uses \(\epsilon\)-greedy \(Q\)-learning in conjunction with
	  rewards that are chosen at random from the interval
	  \([0,1]\) at every time step.
      </li>
      <li>
	<B>\(\epsilon\)Qf</B> — <I>\(\epsilon\)-greedy \(Q\)-learning
	  with fitness-based reward.</I> This type of agent uses
	  \(\epsilon\)-greedy \(Q\) learning with a reward defined
	  exclusively in terms of fitness increments or decrements. A
	  food item carries a reward of \(10\); a poison item of
	  \(-10\).
	</li>
      </ul>
    </P>
  
</DIV>



<div  CLASS="slide">
  <h1>environments</h1>

  <P>
    The three types of environments used in our experiments:
    <ul>
      <li>
	<I>Left:</I> the map for Experiment 1, which involved
	exploration only (no food resources). Thick black lines are
	impassable walls.
      </li>
      <li>
	<I>Middle:</I> the map for Experiments 2 and 3,
	in which food could be found in the four boxes situated in
	the corners.
      </li>
      <li>
	<I>Right:</I> in Experiment 4, the \(100\times 100\) map
	contained small (\(3\times 3\)) food patches, shown in
	red, and also some poison patches, shown in blue.
      </li>
    </ul>
  </P>
  <table>
    <tr>
      <td><img src="Gao16b-fig1a.png" width=200></td>
      <td><img src="Gao16b-fig1b.png" width=200></td>
      <td><img src="Gao16b-fig1c.png" width=200></td>
      </tr>
      </table>
  
</DIV>



<div  CLASS="slide">
  <h1>Experiment 1: fitness vs. time</h1>

  <img src="Gao16b-fig2.png" class="figure-right">
    <P>
      Our first goal was to test the newly introduced types of
      intrinsic reward function in a pure exploration task, in the
      same setting as investigated in previous IMRL studies.
    </P>
    <P>
      Mean cumulative fitness (number of squares explored) vs. time,
      for four types of agents: Qc (\(Q\)-learning with combined
      well-being reward); Qh (\(Q\)-learning with hedonic reward);
      \(\epsilon\)Qwb (\(\epsilon\)-greedy \(Q\)-learning with
      well-being reward); and \(\epsilon\)Qr (\(\epsilon\)-greedy
      \(Q\)-learning with random reward).
    </P>
    <P>
      Agents of type Qc were the fastest in attaining the maximum
      fitness. The maximum possible value of cumulative fitness in this
      task was 36. Each curve is an average of 100 runs.
    </P>
    <P>
      <B>Using combined happiness as the reward results in fastest
      exploration.</B>
    </P>
    
</DIV>



<div  CLASS="slide">
  <h1>Experiment 2: fitness vs. time, with foraging</h1>

  <img src="Gao16b-fig3.png" class="figure-right">
    <P>
      Next, we tested the performance of the different reward function
      (agent) types in a foraging task, which involved both the
      exploration of the environment and the search for food.
    </P>
    <P>
      Mean cumulative fitness vs. time, for five types of agents: Qc,
      Qh, Qf, \(\epsilon\)Qwb, and \(\epsilon\)Qr. Agents of type Qc
      (\(Q\)-learning with combined well-being reward) performed the
      best. Each curve is an average of 100 runs.
    </P>
    <P>
      <B>Using combined happiness as the reward results in the most
      effective foraging.</B>
    </P>

  
</DIV>



<div  CLASS="slide">
  <h1>Experiment 2, spatial exploration</h1>

  <img src="Gao16b-fig4.png" class="figure-right">
  <P>
    A log frequency plot that shows the number of times each location
    has been visited by the agent in its lifetime. The food boxes in
    this environment are located in the upper and lower left
    corners. The three types of agents, from left to right, are: Qh,
    Qc, and \(\epsilon\)Qf.
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>Experiment 3: more challenging environments</h1>

  <img src="Gao16b-fig5.png" class="figure-right">
  <P>
    In this experiment, we aimed to demonstrate that, unlike fitness-based
    reward functions, which typically require hand-tuning, the
    happiness-based intrinsic reward formulation is flexible enough to
    perform well also in more challenging environments than the previous
    one.
  </P>
  <P>
    Mean cumulative fitness vs. time, for five types of agents: Qc,
    Qh, Qf, \(\epsilon\)Qwb, and \(\epsilon\)Qr. Agents of type Qc
    performed the best.
  </P>
  <P>
    <B>In the more challenging environment too, using combined
      happiness as the reward results in the most effective
      foraging.</B>
  </P>

  
</DIV>



<div  CLASS="slide">
  <h1>Experiment 3, spatial exploration</h1>

  <img src="Gao16b-fig6.png" class="figure-right">
  <P>
    A log frequency plot that shows the number of times each location
    has been visited by the agent in its lifetime. The food boxes in
    this environment are located in the upper and lower left
    corners. The three types of agents, from left to right, are: Qh,
    Qc, and \(\epsilon\)Qf.
  </P>
  
</DIV>



<div  CLASS="slide">
  <h1>Experiment 4: a large (100 x 100) environment</h1>

  <img src="Gao16b-fig7.png" class="figure-right">
  <P>
    Unlike most of the simulation studies of IMRL, which involve
    environment maps not larger than \(10 \times 10\), this experiment
    is situated on a \(100 \times 100\) grid, allowing us to
    investigate the scaling properties of the IMRL formulations under
    consideration. The foraging problem here is also more realistic in
    that it involves both positive and negative rewards, which is the
    first step toward accounting for approach vs. avoidance behaviors.
  </P>
  <P>
    Mean cumulative fitness vs. time, for five agent types: Qc,
    \(\epsilon\)Qwb, Qf, Qh, and \(\epsilon\)Qr. Agents of type Qc
    outperform others in learning to accumulate food while avoid
    poison.
  </P>
  <P>
    <B>Even on the large \(100 \times 100\) grid, using combined
      happiness as the reward results in the most effective
      foraging.</B>
  </P>

  
</DIV>


<div  CLASS="slide">
  <h1>summary</h1>

  <P>
    In four experiments, in which simulated agents learned to explore
    and forage in simulated environments, we show that <I>agents whose
    reward function properly balances momentary (hedonic) and
    longer-term (eudaimonic) well-being outperform agents equipped
    with standard fitness-oriented reward functions</I>. Our findings
    suggest that happiness-based features can be useful in developing
    robust, general-purpose reward mechanisms for intrinsically
    motivated autonomous agents.
  </P>
  <P class="incremental">
    In natural foraging situations, biological agents must maintain
    such a balance to survive. The results of Experiments 3 and 4, in
    which the best performance was achieved by agents that blended
    short-term (hedonic) and long-term (eudaimonic) intrinsic rewards,
    suggest <B>a possible role for the components of happiness in
    playing off novelty against current achievement</B> — a finding
    that is also consistent with our earlier results involving
    evolutionary agentbased simulations (Gao & Edelman, 2016), as well
    as with a broader set of philosophical and psychological
    considerations (Edelman, 2012).
  </P>

</div>
  

<!--

<div  CLASS="slide">
  <h1>comparison of fitness outcomes</h1>

  <img src="Gao16b-tab2.png" width=100%>
  <P>
    Fitness outcomes for different experiments and agent types. The
    cumulative fitness values means and 95% confidence intervals over
    100 runs.
  </P>
  <P>
    A comparison of the mean cumulative fitness attained by agents
    equipped with the different reward functions across all four of
    our experiments indicates a statistically significant advantage of
    reward functions based on the components of well-being vs. those
    based on fitness.
  </P>
  
</DIV>

  -->

  
<div class="footer">
<p>Last modified: Tue Aug 11 2020 at 22:44:26 EDT</p>
</div>
</body>
</html>
