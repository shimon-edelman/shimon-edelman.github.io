<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Psych 3140/6140 wk-13-2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<!-- <meta name="copyright" content="Copyright &169; 2008-2024 Shimon Edelman"/> -->
<meta name="font-size-adjustment" content="-1" /> <!-- DEFAULT SIZE -->
<link rel="stylesheet" href="../Slidy/w3c-blue3.css"
 type="text/css" media="screen, projection, print" />
 <link rel="stylesheet" href="extras.css"
 type="text/css" media="screen, projection, print" />
<script src="../Slidy/slidy.js" type="text/javascript">
</script>
<script type="text/javascript"
  src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>

<!-- 
<rdf:RDF xmlns="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<License rdf:about="http://creativecommons.org/licenses/by-sa/2.5/">
<permits rdf:resource="http://creativecommons.org/ns#Reproduction"/>
<permits rdf:resource="http://creativecommons.org/ns#Distribution"/>
<requires rdf:resource="http://creativecommons.org/ns#Notice"/>
<requires rdf:resource="http://creativecommons.org/ns#Attribution"/>
<permits rdf:resource="http://creativecommons.org/ns#DerivativeWorks"/>
<requires rdf:resource="http://creativecommons.org/ns#ShareAlike"/>
</License>
</rdf:RDF>
-->

<!-- this defines the slide background -->

<div class="background">
  <div class="header">
  <!-- sized and colored via CSS -->
  </div>
  <!-- hidden style graphics to ensure they are saved with other content -->
  <img class="hidden" src="../Slidy/bullet.png" alt="" />
  <img class="hidden" src="../Slidy/fold.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold.bmp" alt="" />
  <img class="hidden" src="../Slidy/fold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/nofold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-nofold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold-dim.gif" alt="" />

  <div class="footer">
  <!-- modify the following text as appropriate -->
  Week 13.2 &#151;
  </div>
</div>

<!-- COVER PAGE SLIDE -->
<div class="slide cover">
  <div class="header">
    <h1>Psych 3140/6140</h1>
    <p><a href="http://shimon-edelman.github.io">Shimon Edelman</a>,
    &lt;<a href="mailto:se37@cornell.edu">se37@cornell.edu</a>&gt;</p>
  </div>
  <div style="float:left">
    <h2>Week 13: advanced topics I</h2>
    <h3>&nbsp;Lecture 13.2: real-world Bayes</h3>
  </div>
  <img src="../Lake-Michigan-horizon.jpg" title="Computing the Mind"
  class="figure-right"  height=70%>

</div>
<!-- END COVER PAGE -->



<div  CLASS="slide">
  <h1>real-world Bayes</h1>

  <img src="magic-realism.jpg" class="figure-right"
       title="Come on, honey, enough with the magic realism"
       height=50%>
  <P>
    <B>Getting real about being Bayesian</B> (an algorithmic take):
  </P>
  <P>
    <I>Bayesian brains without probabilities</I>, A. N. Sanborn and
    N. Chater, Trends in Cognitive Sciences 20:883-893  (December 2016).
  </P>
  
</div>



<div  CLASS="slide">
  <h1>Bayesian intractability</h1>

  <P>
    Notwithstanding the empirical success of the Bayesian framework,
    models formulated within this framework are known to often face
    the theoretical obstacle of computational intractability.
    Formally, this means that computations that are postulated by many
    Bayesian models of cognition fall into the general class of
    so-called <a href="https://en.wikipedia.org/wiki/NP-hardness"
    target=new>NP-hard problems</a>. Informally, this means that the
    computations postulated by such models are too resource demanding
    to be plausibly performed by our resource-bounded minds/brains in
    a realistic amount of time for all but small inputs.
  <P>
    <small>
      <I>Bayesian intractability is not an ailment that approximation
	can cure</I>, Johan Kwisthout, Todd Wareham, Iris van Rooij
      (2011). Cognitive Science 35(5):779-84.
    </small>
  </P>
  </P>

</div>

  

<div  CLASS="slide">
  <h1>Bayesian intractability</h1>

  <P>
    <font color=gray>
    Notwithstanding the empirical success of the Bayesian framework,
    models formulated within this framework are known to often face
    the theoretical obstacle of computational intractability.
    Formally, this means that computations that are postulated by many
    Bayesian models of cognition fall into the general class of
    so-called <a href="https://en.wikipedia.org/wiki/NP-hardness"
    target=new>NP-hard problems</a>. Informally, this means that the
    computations postulated by such models are too resource demanding
    to be plausibly performed by our resource-bounded minds/brains in
    a realistic amount of time for all but small inputs.
    </font>
  </P>
  <P>
    NP-hard problems are problems with the property that they can be
    solved only by superpolynomial time algorithms. Such algorithms
    require an amount of time which cannot be upper bounded by any
    polynomial function \(n^c\) (where \(n\) is a measure of the input
    size and \(c\) is some constant). Examples are exponential-time
    algorithms, which require a time that can, at best, be upper
    bounded by some exponential function \(c^{n}\).
  </P>
  <P>
    To see that such algorithms consume an excessive amount of time,
    even for medium input size, consider that \(2^{25}\) is more than
    the number of seconds in a year and \(2^{35}\) is more than the
    seconds in a millennium. To the extent that the cognitive
    abilities that Bayesian models aim to describe operate on a time
    scale of seconds or minutes, computations requiring on the order
    of years or centuries for their completion are inevitably
    explanatorily unsatisfactory, no matter how well the models may
    fit human performance data obtained in the laboratory.
  </P>
  <P>
    <small>
      <I>Bayesian intractability is not an ailment that approximation
	can cure</I>, Johan Kwisthout, Todd Wareham, Iris van Rooij
      (2011). Cognitive Science 35(5):779-84.
    </small>
  </P>

</div>

  

<div  CLASS="slide">
  <h1>Bayesian intractability</h1>

  <P>
    To make our points, we will use a widely adopted—see, for example, Baker et al.,
    (2009), Chater and Manning (2006), Yuille and Kersten (2006)—subcomputation of cognitive
    Bayesian models as an illustrative example: probabilistic
    <a href="https://en.wikipedia.org/wiki/Abductive_reasoning"
    target=new>abduction</a>, a.k.a. most probable explanation
    (MPE). In brief, this computation is defined by the following
    input–output mapping:
    <DIR>
      <P>
	<SC>Most Probable Explanation (MPE)</SC>
      </P>
      <P>
	<I>Input:</I> A set of hypotheses \(H\), a set of observations
	\(E\), and a knowledge structure \(K\) encoding the
	probabilistic dependencies between observations, hypotheses,
	and possibly intermediate variables (e.g., \(K\) could be a
	Bayesian network).
      </P>
      <P>
	<I>Output:</I> A truth assignment for each hypothesis in \(H\)
	with the largest possible conditional probability over all
	such assignments (more formally,
	\(\textrm{argmax}_{T(H)}Pr_K\left(T\left(H\right)\mid
	E\right)\) where \(T\) is a function \(T : H \rightarrow
	\{true, false\}\)).
      </P>
    </DIR>
  </P>
  <P>
    <small>
      <I>Bayesian intractability is not an ailment that approximation
	can cure</I>, Johan Kwisthout, Todd Wareham, Iris van Rooij
      (2011). Cognitive Science 35(5):779-84.
    </small>
  </P>

</div>

  

<div  CLASS="slide">
  <h1>Bayesian intractability</h1>

  <P>
    The computational complexity of MPE has been extensively studied
    in the computer science literature. Not only is it known that
    computing MPE is NP-hard (Shimony, 1994), but it is also known
    that "approximating" MPE — in the sense of computing a truth
    assignment that has close to maximal probability — is NP-hard.
  </P>
  <P>
    An even more sobering result is that it has been proven NP-hard to
    compute a truth assignment with a conditional probability of at
    least \(q\) for any value \(0 < q < 1\).  Importantly, such
    inapproximability results hold not only for MPE but also for many
    other computations postulated in Bayesian models.
  </P>
  <P class="incremental">
    A general methodology for identifying restricted domains of inputs
    for which otherwise intractable computations are tractable is
    available and builds on the mathematical theory of
    <a href="https://en.wikipedia.org/wiki/Parameterized_complexity"
    target=new>parameterized complexity</a>.
  </P>
  <P>
    <small>
      <I>Bayesian intractability is not an ailment that approximation
	can cure</I>, Johan Kwisthout, Todd Wareham, Iris van Rooij
      (2011). Cognitive Science 35(5):779-84.
    </small>
  </P>

</div>


  

<div  CLASS="slide">
  <h1>Bayesian Sampler vs. ideal Bayesian reasoning (Sanborn and Chater, 2016)</h1>

  <img src="flounder-checkerboard.jpg" class="figure-right">
  <P>
    "Bayesian explanations have swept through cognitive science over the past
    two decades, from intuitive physics and causal learning, to perception,
    motor control and language. Yet
    <a href="https://twitter.com/jadehiggs94/status/1363188080451219461"
    target=new>people flounder</a> with even the simplest probability
    questions. What explains this apparent paradox?"
  </P>
  <P class="incremental">
    "Bayesian brains need not represent or calculate probabilities at all and
    are, indeed, poorly adapted to do so. Instead, the brain is a
    <a href="https://twiecki.github.io/blog/2015/11/10/mcmc-sampling/" target=new>Bayesian
      sampler</a>."
  </P>
  <P class="incremental">
    "The key insight:
    <BR>
      although explicitly representing a probability distribution is
      hard, drawing samples from it is relatively easy. Sampling does
      not require knowledge of the entire distribution. It can work
      merely with a LOCAL sense of RELATIVE
      PROBABILITIES. Intuitively, we have this local sense: once we
      ‘see’ a solution, it is often easy to see that it is better than
      another one, even if we cannot exactly say what either
      probability is. By continually sampling, we slowly build up a
      picture of <strike>all</strike> <I><B>most</B></I> of the
      possibilities. Using a number of samples much smaller than the
      number of hypotheses makes the computations feasible."
    </P>

</div>



<div  CLASS="slide">
  <h1>the android metaphor</h1>

  <img src="SanbornChater16-fig1A.png" class="figure-right" height=40%>
  <P>
    "Sampling algorithms have difficulties with isolated modes and produce
    <a href="https://en.wikipedia.org/wiki/Autocorrelation"
    target=new>autocorrelations</a>.
  </P>
  <P>
    In this illustration, the android climbs the landscape of the (log)
    posterior probability distribution. The android uses the difference in
    height of its two feet to decide where to step, and its location is tracked
    over time (red <font color=red>x</font>). A histogram of its locations
    after many steps matches the mode of the probability distribution it
    explored."
  </P>
  
</div>


<div  CLASS="slide">
  <h1>comparing sampling methods on 2D distributions</h1>

  <img src="SanbornChater16-fig1B.png" class="figure-right" height=85%>
  <P>
    "Each row is a different example probability distribution (two
    unimodal and two bimodal ones). The first column shows a
    topographic map of the posterior density. The second and third
    columns illustrate samples drawn using
    the <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm"
    target=new>Metropolis-Hastings</a> algorithm
    and <a href="https://en.wikipedia.org/wiki/Just_another_Gibbs_sampler"
	   target=new>JAGS</a> ("Just Another
    <a href="https://en.wikipedia.org/wiki/Gibbs_sampling"
       target=new>Gibbs Sampler</a>") program 
    respectively. Within each column are trace plots that show how the
    location of the sampler changes along each variable during each iteration
    of the sampling process.
  </P>
  <P>
    Autocorrelations are present when a sample depends on the value of
    the previous sample in the trace plots (e.g., Metropolis-Hastings
    in the second row). Also shown are bivariate scatterplots that can
    be used to compare the samples obtained against the true
    distributions in the first column. These show that not all of the
    modes are always sampled, even when thousands of samples are drawn
    (i.e., in the bottom row)."
  </P>
  
</div>




<div  CLASS="slide">
  <h1>Bayesian Sampler vs. ideal Bayesian reasoning</h1>

  <img src="SanbornChater16-tab1.png" class="figure-right" height=80%>
  <P>
    "Bayesian cognitive models [using sampling instead of full-blown
    probability estimation] that operate well in complex domains
    actually predict probabilistic reasoning errors in simple
    domains."
  </P>
  <HR>
  <P>
    This is good news for the research program that attempts to
    explain reasoning and
    <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2021.584689/full"
    target=new><B>cognitive illusions</B></a> in terms of a Bayesian
    model.
  </P>
  
</div>



<div  CLASS="slide">
  <h1>why the CONJUNCTION FALLACY arises from a Bayesian sampler</h1>

  <img src="SanbornChater16-fig2A.png" class="figure-right" height=85%>
  <P>
    "The top row illustrates a query about one piece of the puzzle.
    The bottom row illustrates that evaluating the probability of a
    conjunction is easier.
  </P>
  <P>
    <I>Top:</I> constituent question —<BR> What is the probability that
    the piece outlined in <font color=red>red</font> is in the correct
    position in the frame?
  </P>
  <P>
    <I>Bottom:</I> CONJUNCTION question —<BR> What is the probability that
    all of the pieces are in the correct positions in the frame?
  </P>
  <P class="incremental">
    <B>The correct locations of all the puzzle pieces cannot, of
      course, be more probable that the correct location of a single
      piece.</B> Yet when considered in isolation, the evidence that
      an isolated piece is correct is weak (from a SAMPLING
      standpoint, it is not clear whether, e.g., swapping pieces leads
      to a higher or lower probability). But in the fully assembled
      puzzle (i.e., the ‘peak’ in probability space), local
      comparisons are easy – switching any of the pieces would make
      the fit worse – so you can be nearly certain that all the pieces
      are in the correct position. So the whole puzzle will be judged
      more probable than a single piece, exhibiting the
      <a href="https://en.wikipedia.org/wiki/Conjunction_fallacy"
      target=new><B>conjunction fallacy</B></a>."
  </P>
  
</div>


  
<div  CLASS="slide">
  <h1>why BASE-RATE NEGLECT arises from a Bayesian sampler</h1>

  <img src="SanbornChater16-fig2B.png" class="figure-right" height=70%>
  <P>
    "Local assessments of relative probability are easy. Comparing the
    probability of seeing the two astronomical events in a year, or
    the probability of the two quotations appearing on a random
    website, are both relatively easy."
  </P>
  <P>
    "Comparing the probability of seeing one of the astronomical
    events in a year to the probability of seeing one of the
    quotations on a random website is more difficult. In particular,
    when
    comparing <a href="https://www.poetryfoundation.org/poems-and-poets/poems/detail/43290"
    target=new>"Things fall apart; the centre cannot hold"</a> to the
    eclipse, the quote may seem more likely as it is a common among
    quotes, yet this
    <a href="https://en.wikipedia.org/wiki/Base_rate_fallacy"
    target=new><B>neglects the base rates</B></a>: most websites do
    not have literary quotations, and there are many chances for an
    eclipse each year."
  </P>
  
</div>


<div  CLASS="slide">
  <h1>sampling and task richness</h1>

  <img src="all-white-puzzle.jpg" class="figure-right" height=70%>
  <P>
    "If our brains do not respect the laws of probability for simple
    tasks, surely the Bayesian approach to the mind must fail in rich
    domains such as vision, language and motor control with huge data
    and hypothesis spaces."
  </P>
  <P>
    "Viewing brains as sampling from complex probability distributions
    upends this argument. Rich, realistic tasks, in which there is a
    lot of contextual information available to guide sampling, are
    just those where the Bayesian sampler is most effective. <B>Rich
    tasks focus the sampler on the areas of the probability landscape
    that matter – those that arise through experience</B>. By limiting
    the region in which the sampler must search, rich problems can
    often be far easier for the sampler than apparently simpler, but
    more abstract, problems."
  </P>

</div>


<div  CLASS="slide">
  <h1><I>[a propos]</I> puzzles...</h1>

  <img src="circle-A-graffito-from-Bologna.jpg" class="figure-right"
       height=85%>
    <P>
      The <a href="https://theanarchistlibrary.org/library/ruth-kinna-annotated-bibliography-on-anarchism"
      target=new>missing piece</a> of the puzzle of life.
    </P>

</div>

  
<div  CLASS="slide">
  <h1>a "good enough" approach</h1>

  <img src="satisficing-1.png" class="figure-right" height=70%>
  <P>
    "Moreover, the problem of learning the structure of the world, or
    interpreting an image or a sentence, involves
    finding <a href="https://en.wikipedia.org/wiki/Satisficing"
    target=new>‘good-enough’</a> hypotheses to usefully guide our
    actions, which can be achieved by local sampling in the
    probability landscape. Such hypotheses are no less valuable if an
    isolated peak, corresponding to an even better hypothesis,
    remained undiscovered."
  </P>
  <P class="incremental">
    ["Лучшее — враг хорошего"]
  </P>

</div>



<div  CLASS="slide">
  <h1>is local sampling good enough?</h1>

<!--   <img src="" class="figure-right" height=400> -->
  <P>
    "We suggest too that, for many real-world problems, multiple but
    distant peaks, corresponding to very different hypotheses about
    the world, may be rare, particularly when context and background
    knowledge are taken into account. Language is locally ambiguous,
    but it is very unlikely that the acoustic signal of a whole
    sentence in English happens to have an equally good interpretation
    in Latin [...]"
  </P>

</div>


<div  CLASS="slide">
  <h1>is local sampling good enough?</h1>

<!--   <img src="wk-14-1.html" class="figure-right" height=400> -->
  <P>
    "We suggest too that, for many real-world problems, multiple but
    distant peaks, corresponding to very different hypotheses about
    the world, may be rare, particularly when context and background
    knowledge are taken into account. Language is locally ambiguous,
    but it is very unlikely that the acoustic signal of a whole
    sentence in English happens to have an equally good interpretation
    in Latin [...]"
  </P>
  <P>
  Italian/Latin: I VITELLI DEI ROMANI SONO BELLI
  </P>
  <P>
  Latin meaning (Italian paraphrase): và, o Vitellio, al suono di guerra del dio Romano
  </P>
  <P>
  Latin meaning (English translation): go, Vitellus, to the sound of the Roman god of war
  </P>
  <P>
  Italian meaning (Latin paraphrase): ROMANORUM VITULI PULCHRI SUNT
  </P>
  <P>
  Italian meaning (English translation): the calves of the Romans are beautiful
  </P>
  <P>
    <I>[This also illustrates how written words fails to capture the
      richness of natural language.]</I>
  </P>
  
</div>


<div  CLASS="slide">
  <h1>is local sampling good enough?</h1>

  <a href="http://www.ada.auckland.ac.nz/" target=new><img src="Ada.jpg" class="figure-right"></a>
  <P>
    "We suggest too that, for many real-world problems, multiple but
    distant peaks, corresponding to very different hypotheses about
    the world, may be rare, particularly when context and background
    knowledge are taken into account. Language is locally ambiguous,
    but it is very unlikely that the acoustic signal of a whole
    sentence in English happens to have an equally good interpretation
    in Latin [...]"
  <DIR><DIR>
    <I>
    Ce beau jardin fleurit en mai,<BR>
    Mais en hiver<BR>
    Jamais, jamais, jamais, jamais, jamais<BR>
	    N’est vert, n’est vert, n’est vert, n’est vert,
	    <a href="https://www.folger.edu/explore/shakespeares-works/king-lear/read/5/3/"
	    target=new>n’est vert</a><BR> 
    </I>
  </DIR></DIR>
  <P>
  <HR>
  <P>
    [See <a
	   href="https://www.jstor.org/stable/25748172?seq=1"
	   target=new>here</a> for a discussion of multilingual word-play in
    <a href="https://en.wikipedia.org/wiki/Vladimir_Nabokov" target=new>Nabokov</a>'s novel
    <a href="https://www.ada.auckland.ac.nz/" target=new><I>Ada, or
	Ardor</I></a>.]
  </P>
    
</div>


<div  CLASS="slide">
  <h1>is local sampling good enough?</h1>

  <a href="http://archive.thedali.org/mwebcgi/mweb.exe?request=record;id=123;type=101" target=new><img src="Dali-slave-market.gif"
  class="figure-right" title="a painting by S. Dalí" height=85%></a>
  <P>
    <font color=gray>
      "We suggest too that, for many real-world problems, multiple but
      distant peaks, corresponding to very different hypotheses about
      the world, may be rare, particularly when context and background
      knowledge are taken into account. Language is locally ambiguous,
      but it is very unlikely that the acoustic signal of a whole
      sentence in English happens to have an equally good
      interpretation in Latin;</font> vision, too, is locally
      ambiguous but the probability that a portrait photograph could
    equally be reinterpreted as a rural scene is infinitesimal."
  </P>
  
</div>



<div  CLASS="slide">
  <h1>is local sampling good enough?</h1>

  <P>
    <font color=gray>
      "We suggest too that, for many real-world problems, multiple but
      distant peaks, corresponding to very different hypotheses about
      the world, may be rare, particularly when context and background
      knowledge are taken into account. Language is locally ambiguous,
      but it is very unlikely that the acoustic signal of a whole
      sentence in English happens to have an equally good
      interpretation in Latin; vision, too, is locally ambiguous but
      the probability that a portrait photograph could equally be
      reinterpreted as a rural scene is infinitesimal."
    </font>  
  </P>
  <P>
    "In complex real-world problems, then, climbing a rugged
    probability landscape to find ‘good-enough’ hypothesis is crucial;
    linking to numerical probabilities, even approximately, is
    not. Thus, the view of cognition
    as <a href="https://en.wikipedia.org/wiki/Satisficing"
    target=new>satisficing</a> need not be viewed as opposed to the
    Bayesian approach. Rather, Bayesian sampling provides a mechanism
    for satisficing in real-world environments."
  </P>

</div>



<div  CLASS="slide">
  <h1>summing up: the Stones weigh in</h1>

  <img src="satisficing-2.jpg" class="figure-right" height=70%>
  <P>
    <a href="https://www.youtube.com/watch?v=jv9sDn_2XkI" target=new>You can't</a> always get what you want<BR>
      You can't always get what you want<BR>
	You can't always get what you want<BR>
	  But if you try sometimes well you might find<BR>
	    You get what you need
	  </P>

</div>

	

<div  CLASS="slide">
  <h1>an AI-opener: AI-by-learning from sampled behaviors [copied from Lecture 8.2]</h1>

  <img src="vanRooij23-fig2.png" class="figure-right" height=65%>
  <P>
    A visual illustration of the hypothetical learning scenario and
    its formalisation: Dr. Ingenia has access (magically and at no
    cost) to any machine learning method \(\mathbb{M}\), present or
    future, and by repeatedly sampling data \(D\) from the
    distribution \(\cal{D}\) they can use whatever \(\mathbb{M}\)
    they like to create a program \(L_{A}\) that when implemented and
    run generates behaviours \(b = A(s)\) when prompted by different
    situations \(s\).
  </P>
  <P>
    The goal is to generate with non-negligible probability
    \(\delta(n)\) an algorithm \(A\) that behaves (approximately)
    human-like, in the sense that \(A\) is non-negligibly
    (\(\epsilon(n)\)) better than chance at picking behaviours that
    are possible for \(s\) in \(\cal{D}\).
  </P>
  <P>
    Here, \(n\) is a measure of the situation complexity, i.e., the
    maximum length of strings (\(\vert s\vert\)) needed to encode the
    relevant information in the situations \(s\). The problem is
    INTRACTABLE if the best possible algorithm for solving it requires
    a number of steps that is EXPONENTIAL in \(n\).
  </P>
  <P>
    <small>
      Iris van Rooij, Olivia Guest, et
      al. (2023). <a href="https://osf.io/preprints/psyarxiv/4cbuv"
      target=new><I>Reclaiming AI as a theoretical tool for cognitive
      science</I></a>. Preprint.
    </small>
  </P>
  
</div>



<DIV CLASS="slide">
  <h1>a reality check: AI-by-learning</h1>

  <img src="infant-surprised-face.jpg" class="figure-right">
  <P>
    <SC>AI-by-Learning</SC> (informal)
  </P>
  <P>
    <I>Given:</I> A way of sampling from a distribution \(\cal{D}\).
  </P>
  <P>
    <I>Task:</I> Find an algorithm \(A\) (i.e., ‘an AI’) that, when
    run for different possible situations as input, outputs behaviours
    that are human-<I>like</I> (i.e., approximately like \(\cal{D}\)
    for some meaning of ‘approximate’).
  </P>
  <P class="incremental">
    <B>Theorem 2</B> (Ingenia Theorem). <SC>AI-by-Learning</SC> is intractable.
  </P>
  <P>
    <small>
      Iris van Rooij, Olivia Guest, et
      al. (2023). <a href="https://osf.io/preprints/psyarxiv/4cbuv"
      target=new><I>Reclaiming AI as a theoretical tool for cognitive
      science</I></a>. Preprint.
    </small>
  </P>

</div>

	
<div class="footer">
<p>Last modified: Thu Apr 25 2024 at 08:11:10 EDT</p>
</div>
</body>
</html>
