<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Psych 3140/6140 w-9-1</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<!-- <meta name="copyright" content="Copyright &169; 2014-2024 Shimon Edelman"/> -->
<meta name="font-size-adjustment" content="-1" /> <!-- DEFAULT SIZE -->
<link rel="stylesheet" href="../Slidy/w3c-blue3.css"
 type="text/css" media="screen, projection, print" />
 <link rel="stylesheet" href="extras.css"
 type="text/css" media="screen, projection, print" />
<script src="../Slidy/slidy.js" type="text/javascript">
</script>
<script type="text/javascript"
  src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>

<!-- 
<rdf:RDF xmlns="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<License rdf:about="http://creativecommons.org/licenses/by-sa/2.5/">
<permits rdf:resource="http://creativecommons.org/ns#Reproduction"/>
<permits rdf:resource="http://creativecommons.org/ns#Distribution"/>
<requires rdf:resource="http://creativecommons.org/ns#Notice"/>
<requires rdf:resource="http://creativecommons.org/ns#Attribution"/>
<permits rdf:resource="http://creativecommons.org/ns#DerivativeWorks"/>
<requires rdf:resource="http://creativecommons.org/ns#ShareAlike"/>
</License>
</rdf:RDF>
-->

<!-- this defines the slide background -->

<div class="background">
  <div class="header">
  <!-- sized and colored via CSS -->
  </div>
  <!-- hidden style graphics to ensure they are saved with other content -->
  <img class="hidden" src="../Slidy/bullet.png" alt="" />
  <img class="hidden" src="../Slidy/fold.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold.bmp" alt="" />
  <img class="hidden" src="../Slidy/fold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/nofold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-nofold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold-dim.gif" alt="" />

  <div class="footer">
  <!-- modify the following text as appropriate -->
  Week 9.1 &#151;
  </div>
</div>

<!-- COVER PAGE SLIDE -->
<div class="slide cover">
  <div class="header">
    <h1>Psych 3140/6140</h1>
    <p>Computing the Mind</p>
  </div>
  <div style="float:left">
    <h2>Week 9: higher cognition, II</h2>
    <h3>&nbsp;Lecture 9.1: reasoning</h3>
  </div>
  <img src="../Lake-Michigan-horizon.jpg" title="Computing the Mind"
  class="figure-right"  height=70%>

</div>
<!-- END COVER PAGE -->



<DIV CLASS="slide">
  <h1>varieties of thinking</h1>

  <a href="https://dsmpublicartfoundation.org/public-artwork/thinker-on-a-rock/" target="new"><img src="Flanagan-thinker.jpg" class="figure-right" title="Thinker on a Rock (Barry Flanagan)"></a>
  <p>
  <br>
  <a href="http://www.theguardian.com/theobserver/2014/apr/20/observer-profile-banksy-street-art"
  target=new><img src="Banksy_Rat_by_arterrorist.jpg" class="figure-right" 
  height=250 title="Banksy!"></a>
  Thinking: not just for humans.
  </p><p>
  </p>
  <ul>
    <li>Varieties of thinking:
    <ul>
      <li>decision making</li>
      <li>REASONING</li>
      <li>problem solving</li>
    </ul>
  </li>
</ul>

</div>


<div  CLASS="slide">
  <h1>regarding reasoning</h1>

  <a href="http://museum.cornell.edu/collections/prints-drawings/old-master-prints-drawings-1800/sleep-reason-produces-monsters"
 target=new><img src="Goya-monstros.jpg" class="figure-right" 
  title="El Sueño de la Razon Produce Monstruos (F. Goya)"></a>
  
  <P>
    Reasoning is important.
  </P>
  <P>
    "<a href="https://www.theguardian.com/news/series/cambridge-analytica-files"
	target=new>Those</a> who can make you believe absurdities can make you commit
    atrocities."
    <DIR><DIR>
	— <a href="http://en.wikipedia.org/wiki/Voltaire"
	     target=new>Voltaire</a> (1694-1778)
    </DIR></DIR>
  </P>
  <HR>
    <P>
      <img src="NYTimes-on-Goya.png" class="figure-left" height=250>
      </P>
  <P>
    It's well worth seeing the rest of Goya's
    <a href="https://loscaprichos.org/42-Thou-who-cannot.html"
    target=new><I>Caprichos</I></a>... 
  </P>
  
</div>
    


<DIV CLASS="slide">
  <h1>when reason fails</h1>

  <img src="medicare.jpg" class="figure-right"
  title="A slogan seen at a Tea Party rally">
  <P>
    "<a href="https://www.theguardian.com/news/series/cambridge-analytica-files"
	target=new>Those</a> who can make you believe absurdities can make you commit
    atrocities."
    <DIR><DIR>
	— <a href="http://en.wikipedia.org/wiki/Voltaire"
	     target=new>Voltaire</a> (1694-1778)
    </DIR></DIR>
  </P>
  <HR>
  <P>
    <I>Right:</I> a slogan seen at a Tea Party rally.
  </P>
  <P class="incremental">
    [Examples of reasoning fallacies in the context of climate change
    coming up later.]
  </P>

</div>




<div  CLASS="slide">
  <h1>to reason well, one must understand how the world works</h1>
  
  <a
    href="https://en.wikipedia.org/wiki/Felix,_qui_potuit_rerum_cognoscere_causas"
    target=new><img src="felix-qui-potuit.jpg" height=60% class="figure-right"></a>
    <P>
      <ul>
	<li>
	  correlation vs. causation
	</li>
	<li>
	  modeling the world: causal knowledge
	</li>
	<P>
	  <li>
	    <a href="http://en.wikipedia.org/wiki/Felix,_qui_potuit_rerum_cognoscere_causas"
	       target=new>FELIX QUI POTUIT RERUM COGNOSCERE CAUSAS</a>
	  </li>
	<P>
	  <li><B>graphical models (Bayes Networks)</B></li>
	  <!--
	      <P>
		<li><font color=gray>Bayes Networks and goal-directed RL</font></li>
		-->
	      </ul>
	</P>
  
</div>




<DIV CLASS="slide">
  <h1>the causal stucture of the world</h1>

  <img src="graphical-models-top.gif" height=50% class="figure-right" >
  <P>
    Knowledge of the CAUSAL STRUCTURE of the world is an <a href="https://arxiv.org/pdf/1801.04016.pdf" 
							    target=new>indispensable prerequisite</a> for:
    <ul>
      <li>intervention-related ("what if?") <SC>foresight</SC>;</li>
      <li>diagnostic <SC>reasoning</SC> (from symptoms to causes);</li>
      <li><SC>counterfactual</SC> reasoning ("what if I had acted
	differently?").</li>
    </ul>
  </P>
  <HR>
    <P>
      A <a href="http://en.wikipedia.org/wiki/Graphical_model"
	   target=new><SC>graphical model</SC></a>, such as the
      <a href="http://en.wikipedia.org/wiki/Bayesian_network" target=new>Bayes
	network</a>, is a probabilistically annotated
      <a href="http://en.wikipedia.org/wiki/Graph_%28mathematics%29"
	 target=new>graph</a> that represents causal relationships among some
      variables pertaining 
      to the state of affairs in the world. As such, it constitutes a <I>structured
	probability distribution</I>.
    </P>
    <P>
      Graphical models may contain "hidden" variables that cannot be
      observed directly, only inferred from measurements (<I>M</I>)
      carried out on the observables [and from interventions, as in
      <a href="https://www.urbandictionary.com/define.php?term=FAFO"
      target=new>FAFO</a>].
    </P>

</div>





<DIV CLASS="slide">
  <h1>[EXTRA] re hidden vs. observable variables: the Markov blanket,
  and surprise minimization</h1>

  <img src="Friston13-fig1-markov-blanket.png" class="figure-right"
       height=80%>
    <P>
      System states can be partitioned into internal states and hidden
      or external states, separated by
      a <a href="https://en.wikipedia.org/wiki/Markov_blanket"
      target=new>Markov blanket</a> — comprising sensory and active
      states.
    </P>
    <P>
      The <I>upper panel</I> shows this partition as applied to
      PERCEPTION - THINKING - ACTION cycle in the brain. The
      <a href="https://en.wikipedia.org/wiki/Free_energy_principle"
      target=new>free energy principle</a> dictates that internal and
      active states minimize a free energy functional of sensory
      states. In other words, <B>brain processes lead to actions that
      minimize sensory surprise</B>.
    </P>
    <P>
      <font color=gray>
      The <I>lower panel:</I> exactly the same dependencies hold for a
      living cell, where the internal states pertaining to
      intracellular variables, the sensory states are those of cell
      membrane receptors, and the active states may correspond to
      the <a href="https://en.wikipedia.org/wiki/Actin"
      target=new>actin</a> <a href="https://en.wikipedia.org/wiki/Microfilament"
      target=new>filaments</a> of
      the <a href="https://en.wikipedia.org/wiki/Cytoskeleton"
      target=new>cytoskeleton</a> (or, in the case of neurons, to
      variables that control spiking or other output activity).
      </font>
    </P>
    <P>
      [From
      <a href="http://dx.doi.org/10.1098/rsif.2013.0475"
      target=new><I>Life as we know it</I></a> (Friston, 2013)</a>]
    </P>

</div>

  
  

<DIV CLASS="slide">
  <h1>working with structured probability distributions (Griffiths & Yuille, 2006)</h1>

  <P>
    [Recall <a href="wk-3-1.html#(12)" target=lec>Lecture 3.1</a>]
    Probabilistic models go beyond 'hypotheses' and 'data'. More
    generally, a probabilistic model characterizes a situation by
    defining the joint distribution for a set of random variables
    (which can then be used to seek data in support of various
    hypotheses).
  </P>
  <P>
    For example, imagine that a friend of yours claims to possess the power of
    psychokinesis. He proposes to demonstrate these powers by flipping a coin,
    and influencing the outcome to produce heads. How would you
    respond?
  </P>
  <P>
    We can express all possible outcomes of the proposed tests, as well as
    their causes, using binary random variables:
    <DIR>
      \(X_1\) — coin being flipped and producing heads
      <BR>
	\(X_2\) — a pencil levitating
	<BR>
	  \(X_3\) — your friend having psychic powers
	  <BR>
	    \(X_4\) — the use of a two-headed coin
	  </DIR>
	</P>
	<P>
	  Any set of of beliefs about these outcomes can be encoded in a joint
	  probability distribution, \(P(X_1, X_2, X_3, X_4)\). For example, the
	  probability that the coin comes up heads (\(X_1 = 1\)) should be higher if
	  your friend actually does have psychic powers (\(X_3 = 1\)).
	</P>

</div>


<DIV CLASS="slide">
  <h1>working with structured probability distributions (Griffiths & Yuille, 2006)</h1>

  <P>
    Once we have defined a joint distribution on \(\{X_i\}\), we can
    <B>REASON</B> about the implications of events involving these variables. For
    example, if flipping the coin produces heads (\(X_1 = 1\)), then the
    conditional probability distribution over the remaining variables is 
    $$
    P(X_2,X_3,X_4 \mid X_1=1) = \frac{P(X_1=1,X_2,X_3,X_4)}{P(X_1=1)}
    $$
    This merely reflects the definition of <a
		    href="http://en.wikipedia.org/wiki/Conditional_probability_distribution"
						      target=new>conditional
    probability</a>: the conditional is defined as the joint, divided
    by the marginal.
  </P>
  <P>
    This equation can be interpreted as an application of Bayes' rule
    (recall
    <a href="wk-3-1.html#(19)"
    target=lec>Lecture 3.1</a>), with
    \(X_1\) being the data, and \(X_2,X_3,X_4\) being the hypotheses.
  </P>
  <HR>
  <P class="incremental">
    In principle, the rules of probability can be used with models involving
    any number of variables. However, two factors can make large probabilistic
    models difficult to use.
  </P>

</div>



<DIV CLASS="slide">
  <h1>working with structured probability distributions (Griffiths & Yuille, 2006)</h1>

  <P>
    Why working with multivariate probability distributions is hard:
    <ul>
      <li>
	It is hard to write down a sensible joint distribution over a
	large set of variables.
      </li>
      <li>
	The computational complexity of working with probability
	distributions is exponential in the number of variables
	involved. [This is
	the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality"
	target=new>curse of dimensionality</a>, mentioned
	in <a href="https://shimon-edelman.github.io/Psych-3140/wk-5-1.html#(26)"
	target=new>Lecture 5.1</a>.] 
      </li>
    </ul>
  </P>
  <P>
    A probability distribution over four binary random variables requires
    \(2^4-1 = 15\) numbers to specify, which might seem quite reasonable. If
    we double the number of random variables to eight, we would need to
    provide \(2^8-1 = 255\) numbers.
  </P>

</div>


<DIV CLASS="slide">
  <h1>working with structured probability distributions (Griffiths & Yuille, 2006)</h1>

  <img src="DAG.png" class="figure-right" height=60%>
  <P>
    To keep the computational complexity in check, we can use <B>directed
      graphical models</B>, also known as Bayesian networks or Bayes
    nets.
  </P>
  <P>
    The directed graph used in a Bayes net has one node for each random
    variable in the associated probability distribution. The directed edges
    express the statistical dependencies between the variables in a fashion
    consistent with the
    <a href="https://en.wikipedia.org/wiki/Causal_Markov_condition" target=new><I>Markov condition</I></a>:
    <DIR> <DIR> <DIR> a node is conditionally independent of the rest
	  of the network, given its Markov blanket. [This is to say,
	  when conditioned on its parents, each variable is
	  independent of all other variables except its descendants.]
    </DIR>  </DIR>  </DIR>
  </P>
  <HR>
    <P>
      A reminder: by definition, \(A\) and \(B\) are independent, if
	\(P\left(A,B\right) = P\left(A\right) P\left(B\right)\).
      <BR>
      Therefore, \(A\) and \(B\) are CONDITIONALLY independent given \(C\), if \(P\left(A,B\mid
      C\right) = P\left(A\mid C\right) P\left(B\mid C\right)\)
    </P>

</div>



<DIV CLASS="slide">
  <h1>working with structured probability distributions (Griffiths & Yuille, 2006)</h1>

  <img src="GriffithsYuille06-fig2.png" height=40% class="figure-right">
  <P>
    This CONDITIONAL INDEPENDENCE has an important implication: it
    makes it possible to FACTORIZE the joint probability distribution
    into the product of the conditional distribution for each variable
    conditioned on its parents: $$ P(X_1, X_2, \dots, X_m) = \prod_i
    P\left(X_i \mid \textrm{parents}\left(X_i\right)\right) $$
  </P>
  <P>
    <I>Right:</I> A Bayes net for the "psychic friend" example.
    <BR>This graph identifies
      several <B>assumptions</B> about the relationship between the variables
      involved. For example, \(X_1\) and \(X_2\) are assumed to be independent
      given \(X_3\), indicating that once it is known whether or not the
      friend is psychic, the outcomes of the coin flip and the levitation
      experiments are completely unrelated
      (= <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)" 
	    target=new><B>statistically independent</B></a>). By the Markov
      condition, we can write: 
      $$
      P(X_1, X_2, X_3, X_4) = P(X_1 \mid X_3, X_4) \cdot P(X_2 \mid X_3) \cdot P(X_3) \cdot P(X_4)
      $$
      This factorization allows us to use fewer numbers in specifying the
      distribution over these four variables: we only need one number for
      each variable, conditioned on each set of values taken on by its
      parents; here, 8 rather than 15. Furthermore, 
      recognizing the structure in this probability distribution simplifies some
      of the computations we might want to perform. There are several specialized
      algorithms for efficient probabilistic
      <a href="https://en.wikipedia.org/wiki/Bayesian_network#Inference_and_learning"
	 target=new>inference</a> in Bayes nets, which make
      use of the dependencies among variables.
    </P>
  <P class="incremental">
    <B>The graphical notation is also extremely informative: the
    assumptions [of conditional independence] encoded in the graph are
    conveyed by its missing arrows.</B>
  </P>
  

</div>



<DIV CLASS="slide">
  <h1>an example coming up: Comet C/2020 F3 (NEOWISE) & the plague of 2020</h1>

  <img src="Comet&#32;C&#32;2020&#32;F3&#32;NEOWISE.jpg" class="figure-right" height=80%>
    <P>
      Did the comet of 2020 cause the plague of 2020?
    </P>

</div>
    
  

<DIV CLASS="slide">
  <h1>causality, independence, and conditional independence</h1>

  <img src="graphical-models-bottom.gif" class="figure-right" height=60%>
  <P>
    The assumptions encoded in the graph are conveyed by its missing
    arrows, as per the Markov condition.
  </P>
  <HR>
    <P>
      <I>Left:</I> in this two-node graphical model, the variables
      <I>A</I> and <I>B</I> are <SC><a
				      href="http://en.wikipedia.org/wiki/Statistical_independence"
				      target=new>independent</a></SC>
      (no arc between them).
    </P>
    <P>
      <I>Center:</I> <I>B</I> depends on <I>A</I>.
    <P>
      <I>Right:</I> <I>A</I> and <I>B</I> are <SC><a
						    href="http://en.wikipedia.org/wiki/Conditional_independence" target=new>conditionally independent</a></SC>,
      given <I>C</I>.
    </P>
  <HR>
    <P>
      A reminder: by definition, \(A\) and \(B\) are independent, if
	\(P\left(A,B\right) = P\left(A\right) P\left(B\right)\)
      <BR>
      Therefore, \(A\) and \(B\) are <I>conditionally</I> independent given \(C\), if \(P\left(A,B\mid
      C\right) = P\left(A\mid C\right) P\left(B\mid C\right)\)
    </P>
  
</div>




<DIV CLASS="slide">
  <h1>concepts and the causal stucture of the world</h1>

  <img src="Car_Key_in_ignition.jpg" class="figure-right" height=80%>
  <P>
    An example from the
    <a href="https://www.eea.europa.eu/publications/exiting-the-anthropocene"
    target=new>Anthropocene</a>: why wouldn't my car start?
  </P>


</div>





<DIV CLASS="slide">
  <h1>concepts and the causal stucture of the world</h1>

  <img src="car-start-Bayes-network.gif" height=65% class="figure-right">
  <P>
    <SC><I>Some</I></SC> of the causal factors relevant to car
    starting...
  </P>

</div>







<DIV CLASS="slide">
  <h1>modes of reasoning in Bayesian graphical models</h1>

  <img src="modes-of-reasoning-no-car-example.gif"
       class="figure-right" height=65%>
  <P>
    <I>LEFT:</I>
  </P>
  <P>
    <SC><a href="https://en.wikipedia.org/wiki/Causality"
	   target=new>causal</a> reasoning</SC> or
    <a href="https://en.wikipedia.org/wiki/Deductive_reasoning"
       target=new>deduction</a>:
    <DIR><DIR>
	[<a href="https://en.wikipedia.org/wiki/Modus_ponens" target=new>modus ponens</a>]<BR>
	  <B>if</B> the battery is dead, <B>then</B> the car won't
	  start; the battery is dead; <B>therefore</B> the car won't start.
      </DIR></DIR>
      <DIR><DIR>
	  [<a href="https://en.wikipedia.org/wiki/Modus_tollens" target=new>modus tollens</a>]<BR>
	    <B>if</B> the battery is dead, <B>then</B> the car won't
	    start; the car starts; <B>therefore</B> the battery isn't
	    dead.
	</DIR></DIR>
      </P>
  <HR>
  <P>
  Compare this with:
  <SC><a
  href="https://en.wikipedia.org/wiki/Evidential_reasoning"
  target=new>evidential reasoning</a></SC> or <a
  href="https://en.wikipedia.org/wiki/Abductive_reasoning"
  target=new>abduction</a> rule:
  <DIR><DIR>
    <B>if</B> the car won't start, <B>then</B> the battery is dead
  </DIR></DIR>
  </P>
  <P class="incremental">
  Of all the modes of reasoning, only deduction is logically valid; <BR>
  deduction is also the least useful reasoning mode in real-world
  situations.
  </P>
  <P class="incremental">
  de-Duh!-ction
  </P>
    
</div>




<DIV CLASS="slide">
  <h1>modes of reasoning in Bayesian graphical models</h1>

  <img src="modes-of-reasoning-no-car-example.gif" class="figure-right" height=65%>
  <P>
    <I>MIDDLE:</I>
  </P>
  <P>
    <SC><a
	  href="https://en.wikipedia.org/wiki/Evidential_reasoning"
	  target=new>evidential reasoning</a></SC> or
    <a href="https://en.wikipedia.org/wiki/Abductive_reasoning"
      target=new>abduction</a> rule: 
    <DIR><DIR>
	<B>if</B> the car won't start AND the radio won't play, <B>then</B> the battery is dead
    </DIR></DIR>
  </P>
  <HR>
    <P class="incremental">
      Note how this allows for build-up of EVIDENCE.
    </P>
  
</div>


<DIV CLASS="slide">
  <h1>modes of reasoning in Bayesian graphical models</h1>

  <img src="modes-of-reasoning-no-car-example.gif" class="figure-right" height=65%>
  <P>
    <I>RIGHT:</I>
  </P>
  <P>
    <SC>explaining away</SC> (which exemplifies
    <a href="http://en.wikipedia.org/wiki/Defeasible_reasoning"
       target=new>defeasible reasoning</a>):
    <DIR><DIR>
	<B>if</B> the car won't start, <B>then</B> the battery is dead
	<BR>
	  BUT
	  <BR>
	    <B>if</B> also the transmission is in "drive", <B>then</B>
	    the battery is probably not dead
	</DIR></DIR>
      </P>
      <HR>
	<P class="incremental">
	  The "BUT" clause exemplifies INTERVENTION (testing the state
	  of the transmission).
	</P>

</div>


<DIV CLASS="slide">
  <h1>Pearl's (2018) "Three Layer Causal Hierarchy"</h1>

  <img src="Pearl18-fig1.png" class="figure-right" height=65%>
  <P>
    Questions at a certain level can only be answered if information
    from that level or higher is available.
    <DIR><DIR>
	— <a href="http://bayes.cs.ucla.edu/jp_home.html" target=new>Judea
	  Pearl</a> (2018). <a href="https://arxiv.org/pdf/1801.04016.pdf" 
			       target=new><I>Theoretical Impediments to Machine Learning With
	    Seven Sparks from the Causal Revolution</I></a>.
      </P>
	
</div>
  

<!--

<DIV CLASS="slide">
  <h1>[EXTRA] a cortical model of Bayesian graphical inference (Litvak & Ullman, 2009)</h1>

  <img src="LitvakUllman09-fig4.png" class="figure-right" height=450>
  <P>
  <BR>
  Neuronal LINCs in the context of the full neuronal inference circuit.
  The full circuit (left) contains six inputs (\(Y_i\)) and six interconnected internal
  nodes (the variables \(X_i\)). Each node is a neuronal LINC (right; large black
  frames). The neuronal LINC is built from populations of LIF neurons (small
  red, black, and dashed rectangles). These populations are combined to form
  several instances of weighted maximization nodes (max frames) and
  summation nodes (sigma frames). The main black frame shows the neuronal
  LINC for the variable \(X_1\). 

</div>



<DIV CLASS="slide">
  <h1>[EXTRA] a cortical model of Bayesian graphical inference (Litvak & Ullman, 2009)</h1>

  <img src="LitvakUllman09-fig5.png" class="figure-right" height=450>
  <P>
  <BR>
  <small>
  An example inference trial. The neuronal inference circuit finds the
  MPE solution over a pairwise-MRF graph with 6 variables, 3 states each, and
  18 edges with random compatibility function. A simulation period of 400 ms
  is plotted. (Top 18 plots) Each row \(X_i S_j\) represents the activity in one output
  excitatory population (25 neurons) of the neuronal LINC associated with the
  state j of variable i (the output of three sigma units in Figure 4). A gray dot
  indicates a spike (the height is the index of the neuron). The red curve represents
  a smoothed version of the mean spike rate in the population on a scale of
  0–60 Hz. A colored segment indicates the preferred state at any give time,
  whose instantaneous mean rate is the maximal among the three states of the
  variable. The preferred states found by the network define an
  instantaneous solution or interpretation of the input. An end point of a
  colored segment (black vertical line) indicates a change of the
  instantaneous interpretation. (Bottom plot) Log likelihood of
  instantaneous interpretations found by the circuit. Each number indicates
  the index of the instantaneous interpretation among all 729 possible ones
  when ordered according to their likelihood. At about 310 ms, the network
  found the MPE, the most likely, solution. t
  </small>
  
</div>

-->


<DIV CLASS="slide">
  <h1>[described in detail in the TEXTBOOK] causal evidential reasoning in rats</h1>

  <img src="Blaisdell06-fig1-left-top.jpg" class="figure-right" height=35%>
  <P>
    An animal learns in an observational
    <a href="http://en.wikipedia.org/wiki/Pavlovian" target=new>Pavlovian</a> learning
    phase that a light cue (<B>L</B>) temporally precedes a tone
    stimulus (<B>T</B>) in some trials and food (<B>F</B>) in others.
  </P>
  <P>
    It thus learns a common-cause model with two effects.
  </P>
  <P>
    It also observes that noise <B>N</B> and food <B>F</B>
    co-occur simultaneously, thus learning to associate them.
  </P>
  <P>
    At this point, hearing the tone <B>T</B> should lead to predicting food
    <B>F</B> (via the common cause <B>L</B>). 
    <BR>
      Hearing the noise <B>N</B> should lead to the same prediction (via the association). 
    </P>
  <P>
    <small>
          <a
	    href="http://www.sciencemag.org/cgi/content/abstract/311/5763/1020?etoc"
	    target=new><I>Causal reasoning in rats</I></a>, A. P. Blaisdell <I>et al.</I>, Science
	  311:1020-1022 (2006).
    </small>
  </P>

</div>


<DIV CLASS="slide">
  <h1>contrasting observation with intervention</h1>

  <img src="Blaisdell06-fig1-left.jpg" class="figure-right" height=70%>
  <P>
    <I>Bottom panel (<SC>Intervening</SC> condition):</I>
  </P>
  <P>
  However, if the animal next learns that a newly introduced
  lever turns on the tone <B>T</B>, it should be more reluctant to predict
  food <B>F</B>.
  </P>
  <P>
    Generating the tone <B>T</B> by means of an alternative cause &#151; the lever
    &#151; does not predict food <B>F</B> because the manipulation of an effect
    does not influence its cause (that is, light <B>L</B>).
  </P>
  <P>
    Not so for generating noise <B>N</B>, which has been learned as a <I>direct</I> cause
    for food <B>F</B>.
  <P>
    <small>
          <a
	    href="http://www.sciencemag.org/cgi/content/abstract/311/5763/1020?etoc"
	    target=new><I>Causal reasoning in rats</I></a>, A. P. Blaisdell <I>et al.</I>, Science
	  311:1020-1022 (2006).
    </small>
  </P>

</div>



<DIV CLASS="slide">
  <h1>do rats know the difference between observation and intervention?</h1>

  <img src="Blaisdell06-fig1-left.jpg" class="figure-right" height=70%>
  <P>
    Finding "a dissociation between <B>'seeing'</B> [<SC>observing</SC> the tone <B>T</B>] and
    <B>'doing'</B> [<SC>intervening</SC> to make <B>T</B> happen by pressing the
    lever] would be remarkable, because in the observational learning phase
    <B>T</B> is positively correlated with the light <B>L</B>."
  </P>
  <P>
    <small>
          <a
	    href="http://www.sciencemag.org/cgi/content/abstract/311/5763/1020?etoc"
	    target=new><I>Causal reasoning in rats</I></a>, A. P. Blaisdell <I>et al.</I>, Science
	  311:1020-1022 (2006).
    </small>
  </P>
  <HR>
    <P class="incremental">
      In other words, once the rats get a chance to cause T by pressing
      a lever, they should reduce their association between T and F —
      because causality requires more than just correlation! 
      <BR>
	Do rats realize that?
      </P>
  
</div>



<DIV CLASS="slide">
  <h1>causal evidential reasoning in rats: "the mind's arrows"</h1>

  <img src="Blaisdell06-fig1.jpg" class="figure-right" height=70%>
    <font color=gray>
      <P>
	(A), <i>top:</I> in separate trials, the rat learns associatively that (i) light
	<B>L</B> causes tone <B>T</B>; (ii) that light <B>L</B> causes food
	<B>F</B>; and (iii) that noise <B>N</B> causes food <B>F</B>.
      </P>
      <P>
	(A), <i>bottom:</I> if in the next phase the rat learns that by pressing a 
	lever it can activate <B>T</B> (an <SC>intervention</SC>), it should
	be less eager to expect <B>F</B> in conjunction with <B>T</B>.
      </P>
  </font>
  <P>
    (B): indeed, in the test phase, the rats poked for food in response to
    <B>T</B> ("Common-Cause"; left) less often in the <SC>Intervene</SC>
    condition than in the <SC>Observe</SC> condition. No Intervene/Observe
    difference was found for the responses to noise <B>N</B> ("Direct-Cause";
    right).
  </P>
  <P>
    <small>
          <a
	    href="http://www.sciencemag.org/cgi/content/abstract/311/5763/1020?etoc"
	    target=new><I>Causal reasoning in rats</I></a>, A. P. Blaisdell <I>et al.</I>, Science
	  311:1020-1022 (2006).
    </small>
  </P>

</div>



<DIV CLASS="slide">
  <h1>[EXTRA] counterfactual reasoning in rats</h1>

  <img src="LaurentBalleine15-rat-counterfactuals.png" class="figure-right" height=75%>
  <P>
    <a href="http://en.wikipedia.org/wiki/Counterfactual_thinking"
       target=new>Counterfactual reasoning</a> has been argued to provide an
    adaptive advantage for human action [more on this in
    <a href="wk-10-2.html" target=l>Lecture 10.2</a>]. Laurent and
    Balleine (2015) show that rats can reason 
    counterfactually. They demonstrate that rats can encode consequences that
    their actions do not produce and can use that information to choose
    between alternative actions.
  </P>
  <HR width=30% align=left>
  <P>
    <small><a
	     href="http://www.sciencedirect.com/science/article/pii/S0960982215002134"
	     target=new><I>Factual and counterfactual action-outcome mappings control
	  choice between goal-directed actions in rats</I></a>, V. Laurent and
      B. W. Balleine, Current Biology 25:1-6 (2015).
    </small>
  </P>

</div>



<!--

<DIV CLASS="slide">
  <h1>deductive reasoning</h1>

  <img src="modus-ponens.jpg">
  <BR>
  See Monty Python's <a
  href="http://en.wikipedia.org/wiki/The_Argument_Skit" target=new>Argument Skit</a>.
  <img src="modus-tollens.jpg">

</div>

-->

<DIV CLASS="slide">
  <h1>on failures of reasoning: the Wason selection task</h1>

  <img src="Wason-selection-task-characters.gif" class="figure-right" height=20%>
  <P>
    The task:
    <DIR>
      Flip cards that would allow you to verify the rule that<P>
	<I><B>If</B> a card has a vowel on one side,
	  <P>
	    <B>then</B> it has an odd number on the other side</I>.
	</DIR>
      </P>
      <HR>
	<P>
	  <ul>
	    <li>The <a href="http://en.wikipedia.org/wiki/Wason_selection_task"
		       target=new>Wason selection task</a> exemplifies deductive
	      reasoning.</li>
	    <li>Typically, 25% of the subjects get it right.</li>
	  </ul>
	</P>
  
</div>


<DIV CLASS="slide">
  <h1>an ECOLOGICALLY relevant version (re-phrasing) of Wason's task</h1>

  <img src="Wason-selection-task-drinks.gif" class="figure-right" height=20%>
  <P>
    You're a bouncer in a bar, and you lose your job unless you enforce the
    following law: <b>if</b> a person is drinking beer, <b>then</b> he must be
    over 20 years old.
  </P>
  <P>
    The cards on the right have information about four people sitting
    at a table in your bar. One side of a card tells what a person is
    drinking, and the other side tells that person's age. Which
    card(s) you definitely need to turn over to see if any of the
    people is breaking the law?
  </P>
    <HR>
      <P>
	<ul>
	  <li>This problem is structurally identical to the one with
	    letters/numbers.
	  </li>
	  <li>75% of college student subjects get the right answer on this
	    version.
	  </li> 
	</ul>
      </P>
  
</div>



<DIV CLASS="slide">
  <h1>logical fallacies and climate change denial</h1>

  <video src="climate-change-denial-reasoning-fallacies.mp4"
	 class="figure-right"
	 controls
	 width=70%>
  </video>
  <P>
    Did someone say ECOLOGY?
  </P>
  <HR>
    <P>
      <small>
	[From
	<a href="https://www.theguardian.com/environment/climate-consensus-97-per-cent/2018/feb/06/humans-need-to-become-smarter-thinkers-to-beat-climate-denial"
	   target=new><I>Humans
	    need to become smarter thinkers to beat climate
	    denial</I></a> (Dana Nuccitelli, <I>The Guardian</I>,
	2/6/2018)]
      </small>
    </P>
  
</div>

    
<!--


<DIV CLASS="slide">
  <h1>combining probabilities in reasoning [A. Tversky & <a href="http://www.nobel.se/economics/laureates/2002/" target=new>D. Kahneman</a> (1982)]</h1>

  Linda is 31 years old, single, outspoken, and very bright. She majored in
  philosophy. As a student, she was deeply concerned with issues of
  discrimination and social justice, and also participated in anti-nuclear
  demonstrations. Please rank the following statements by their probability,
  using 1 for the most probable and 8 for the least probable:
  <ol type=a>
    <li>Linda is a teacher in elementary school.</li>
    <li>Linda works in a bookstore and takes Yoga classes.</li>
    <li>Linda is active in the feminist movement.</li>
    <li>Linda is a psychiatric social worker.</li>
    <li>Linda is a member of the League of Women Voters.</li>
    <li><B>Linda is a bank teller</B>.</li>
    <li>Linda is an insurance sales person.</li>
    <li><B>Linda is a bank teller and is active in the feminist movement</B>.</li>
  </ol>

</div>



<DIV CLASS="slide">
  <h1>conjunction fallacy in combining probabilities</h1>

  <img src="Venn.png" class="figure-right">
  <ol type=a>
    <li>Linda is a teacher in elementary school.</li>
    <li>Linda works in a bookstore and takes Yoga classes.</li>
    <li>Linda is active in the feminist movement.</li>
    <li>Linda is a psychiatric social worker.</li>
    <li>Linda is a member of the League of Women Voters.</li>
    <li><B>Linda is a bank teller</B>.</li>
    <li>Linda is an insurance sales person.</li>
    <li><B>Linda is a bank teller and is active in the feminist movement</B>.</li>
  </ol>
  <HR>
  <B>Naive subjects:</B> 89% judged that (h) was more probable than (f).
  <P>
  <B>Graduate students</B> in the decision science program of the Stanford
  Business School: 85% judged that (h) was more probable than (f).
  
</div>



<DIV CLASS="slide">
  <h1>are frequencies more "ecologically relevant" than probabilities?</h1>

  <P>
  <BR>
  "What was available in the environment in which we evolved was the
  encountered <B>frequencies</B> of actual events &#151; for example, that we were
  successful 5 times out of the last 20 times we hunted in the north
  canyon. Our hominid ancestors were immersed in a rich flow of observable
  frequencies that could be used to improve decision-making, given procedures
  that could take advantage of them. So if we have adaptations for inductive
  reasoning, they should take frequency information as input."
  <DIR>  <DIR>
    <P>
    <a href="http://www.psych.ucsb.edu/people/faculty/cosmides/index.php"
    target=new>L. Cosmides</a> & <a
    href="http://www.anth.ucsb.edu/faculty/tooby/" target=new>J. Tooby</a>
    (1996)
  </DIR></DIR>
  <P>
  <HR>
  <P class="incremental">
  PREDICTION:
  <BR>
  Inductive reasoning performance will be better when the subjects are
  asked to judge <B>a frequency, instead of the probability</B> of a single
  event.
  </P>

</div>



<DIV CLASS="slide">
  <h1>a frequentist version of the "feminist bank teller"</h1>

  Linda is 31 years old, single, outspoken, and very bright. She majored in
  philosophy.  As a student, she was deeply concerned with issues of
  discrimination and social justice, and also participated in anti-nuclear
  demonstrations.
  <P>
  There are 200 people who fit the description above.
  <br>
  How many of them are:
  <ol type=a>
    <li>bank tellers?</li>
    <li>bank tellers and active in the feminist movement?</li>
  </ol>
  <P>
  <HR>
  <P>
  Replication of the <B>original-version</B> experiment: 91% of subjects
  mistakenly judged the feminist bank teller option to be more probable than the bank
  teller option.   
  <P>
  <B>Frequentist version</B>: only 22% of subjects mistakenly judged the feminist bank teller
  option to be more probable than the bank teller option. 
  
</div>

-->



<div class="footer">
<p>Last modified: Tue Mar 19 2024 at 09:28:54 EDT</p>
</div>
</body>
</html>
