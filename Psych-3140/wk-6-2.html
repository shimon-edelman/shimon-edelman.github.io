<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Psych 3140/6140 wk-6-2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<!-- <meta name="copyright" content="Copyright &169; 2014-2024 Shimon Edelman"/> -->
<meta name="font-size-adjustment" content="-1" /> <!-- DEFAULT SIZE -->
<link rel="stylesheet" href="../Slidy/w3c-blue3.css"
 type="text/css" media="screen, projection, print" />
 <link rel="stylesheet" href="extras.css"
 type="text/css" media="screen, projection, print" />
<script src="../Slidy/slidy.js" type="text/javascript">
</script>
<script type="text/javascript"
  src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>

<!-- 
<rdf:RDF xmlns="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<License rdf:about="http://creativecommons.org/licenses/by-sa/2.5/">
<permits rdf:resource="http://creativecommons.org/ns#Reproduction"/>
<permits rdf:resource="http://creativecommons.org/ns#Distribution"/>
<requires rdf:resource="http://creativecommons.org/ns#Notice"/>
<requires rdf:resource="http://creativecommons.org/ns#Attribution"/>
<permits rdf:resource="http://creativecommons.org/ns#DerivativeWorks"/>
<requires rdf:resource="http://creativecommons.org/ns#ShareAlike"/>
</License>
</rdf:RDF>
-->

<!-- this defines the slide background -->

<div class="background">
  <div class="header">
  <!-- sized and colored via CSS -->
  </div>
  <!-- hidden style graphics to ensure they are saved with other content -->
  <img class="hidden" src="../Slidy/bullet.png" alt="" />
  <img class="hidden" src="../Slidy/fold.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold.bmp" alt="" />
  <img class="hidden" src="../Slidy/fold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/nofold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-nofold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold-dim.gif" alt="" />

  <div class="footer">
  <!-- modify the following text as appropriate -->
    Week 6.2 &#151; 
  </div>
</div>

<!-- COVER PAGE SLIDE -->
<div class="slide cover">
  <div class="header">
    <h1>Psych 3140/6140</h1>
    <p><a href="http://shimon-edelman.github.io">Shimon Edelman</a>,
    &lt;<a href="mailto:se37@cornell.edu">se37@cornell.edu</a>&gt;</p>
  </div>
  <div style="float:left">
    <h2>Week 6: Memory II</h2>
    <h3>&nbsp;Lecture 6.2: associative memory and hashing</h3>  
  </div>
  <img src="../Lake-Michigan-horizon.jpg" title="Computing the Mind"
  class="figure-right"  height=70%>

</div>
<!-- END COVER PAGE -->




<div  CLASS="slide">
  <h1>computational aspects of information retrieval</h1>

  <a href="https://en.wikipedia.org/wiki/Droodles"
       target=new><img src="pig-around-barn-droodle.jpg"
       class="figure-right" 
       title="a droodle"></a>
  <P>
  <ul>
    <li>
      Managing a large store of information presents a real
      computational challenge.
    </li>
    <li>
      A real-world behavioral task that illustrates this challenge:
      associative lookup (as in "this reminds me of...").
    </li>
    <li>
      An interlude:
      <ul>
	<li>
	  Computational principles of memory.
	</li>
	<li>
	  A geometric model of memory for sequences of events.
	</li>
      </ul>
    </li>
    <li>
      An algorithm-level approach to associative lookup: hashing.
      <ul>
	<li>locality-sensitive hashing (LSH)</li>
      </ul>
    </li>
    <li>An algorithm-level solution: random projection followed by
      "pigeonholing".
      <ul>
	<li>
	  "projection" onto a set of landmarks (what I called the
		Chorus Transform [marked as "extra" material] in last
		week's <a href="wk-5-1.html" target=s>Lecture 5.1</a>)
	</li>
      </ul>
    </ul>
  </P>
  
</div>




<DIV CLASS="slide">
  <h1>memory lookup as falling into an attractor</h1>

    <table cellspacing=5>
    <tr>
      <td>
	Memory storage: creating <a href="http://en.wikipedia.org/wiki/Attractor"
				    target=new>attractors</a>.
      </td>
      <td>
	Memory recall: falling into an attractor.
      </td>
    </tr>
    <tr>
      <td rowspan=2 align=center>
	<img src="pig-around-barn-droodle.jpg">
	</td>
      <td align=center>
	<img src="energy-landscape-trajectory.gif">
	</td>
      </tr>
      </table>  

</div>



<DIV CLASS="slide">
  <h1>associative lookup as falling into an attractor</h1>

  <table cellspacing=5>
    <tr>
      <td>
	<P>
	  ASSOCIATIVE recall: falling into an attractor <B>from a nearby
	    location</B>.
	</P>
	<P>
	  But how to get to the right neighborhood in the first place?
	</P>
      </td>
      <td rowspan=2 align=center>
	<img src="energy-surface-four.jpg" height=500 title="an energy surface of a system with four attractor states">
      </td>
    </tr>
    <tr>
      <td>
	<P>
	  <img src="pig-around-barn-droodle.jpg" title="a droodle">
	  </P>
      </td>
    </tr>
  </table>  

</div>



<DIV CLASS="slide">
  <h1>vagaries of autoassociative recall</h1>

  <P>
    ASSOCIATIVE recall: falling into an attractor <B>from a nearby
      location</B>.
  </P>
  <P>
    But how to get to the right neighborhood in the first place? And
    what to do if more than one item is retrieved?
  </P>

  <table cellspacing=15 class="figure-right">
    <tr>
      <td rowspan=2 align=center>
	<img src="pig-around-barn-droodle.jpg">
      </td>
      <td>
	<img src="dog-tail.jpg">
      </td>
    </tr>
    <tr>
      <td>
	<img src="pig-tail.jpg">
      </td>
    </tr>
  </table>  
  

</div>



      
<div  CLASS="slide">
  <h1>[implementation-level theory] principles of memory (Chaudhuri & Fiete, 2016)</h1>

  <img src="Chaudhuri16-fig4.png" class="figure-right" height=70%>
    <ol type=a>
      <li>
	In discrete attractor networks, states in the neighborhoods of
	 attractors decay to their attractors and are thus
	 automatically corrected (memory retrieval from partial cues).
      </li>
      <li>
	Top, limits on noise correction. Bottom, sensitivity to
	structural noise.
      </li>
      <li>
	When coding with long transients in a non-persistent system,
	the initial separation between states decays over time
	(converging blue traces). Thus, the ability of a given noise
	strength to confuse two states grows with time. 
      </li>
      <li>
	Complex trajectories in
	<a href="https://en.wikipedia.org/wiki/Hopfield_network"
	target=new>balanced networks</a> can be surrounded by 
	a small region that pulls states back onto the
	trajectory. However these regions seem to be vanishingly small
	with system size.
      </li>
    </ol>
    <HR>
      <P>
	<small>
	  Rishidev Chaudhuri and Ila Fiete
	  (2016). <a href="https://www.nature.com/articles/nn.4237"
		     target=new><I>Computational principles of memory</I></a>, Nature
	  Neuroscience 19(3):394.
	</small>
      </P>
  
</div>



    
<div  CLASS="slide">
  <h1>[AN EXAMPLE: behavioral analysis] a topic model of film episodic memory</h1>

  <img src="Heusser21-fig1.png" class="figure-right" height=80%>
  
    <P>
      <small>
	Andrew Heusser et
	al. (2021). <a href="https://doi.org/10.1038/s41562-021-01051-6"
		       target=new><I>Geometric models reveal behavioural
	    and neural signatures of transforming experiences
	    into memories</I></a>, Nature Human Behavior
	5:909.
      </small>
    </P>
  
</div>
  

  
  
<div  CLASS="slide">
  <h1>trajectories through topic space: dynamics of the episode and recalls (Heusser et al., 2021)</h1>

  <img src="Heusser21-fig6ab.png" class="figure-right" height="50%">
    <ol type=a>
      <li>
	The two-dimensional
	<a href="https://en.wikipedia.org/wiki/Topic_model"
	target=new>topic [space]</a> trajectory taken by the episode
	of <I>Sherlock</I>. Each dot indicates an event <font color=gray>[identified
	using the
	<a href="https://en.wikipedia.org/wiki/Hidden_Markov_model"
	target=new>HMM</a>]</font>. Dot colours denote the order of
	the events (early events are in red, later events are in
	blue), and the connecting lines indicate the transitions
	between successive events.
      </li>
      <li>
	The average two-dimensional trajectory captured by
	participants’ recall sequences, with the same format and
	colouring. Arrows reflect the average transition direction
	through topic space taken by any participants whose
	trajectories crossed that part of topic space; blue denotes
	reliable agreement across participants
      </li>
    </ol>  
    <HR>
      <P>
	<small>
	  Andrew Heusser et
		     al. (2021). <a href="https://doi.org/10.1038/s41562-021-01051-6"
		     target=new><I>Geometric models reveal behavioural
		     and neural signatures of transforming experiences
		     into memories</I></a>, Nature Human Behavior
		     5:909.
	</small>
      </P>
  
</div>
  

  
<div  CLASS="slide">
  <h1>language used in most and least precisely remembered events (Heusser et al., 2021)</h1>

  <img src="Heusser21-fig7c.png" class="figure-right" height=85%>
    <P>
      The set of all episode and recall events is projected onto the
      two-dimensional space derived in the previous figure. The dots
      outlined in black denote episode events (dot size is
      proportional to each event’s average precision). The dots
      without black outlines denote individual recall events from each
      participant.
    </P>
    
    <HR>
      <P>
	<small>
	  Andrew Heusser et
		     al. (2021). <a href="https://doi.org/10.1038/s41562-021-01051-6"
		     target=new><I>Geometric models reveal behavioural
		     and neural signatures of transforming experiences
		     into memories</I></a>, Nature Human Behavior
		     5:909.
	</small>
      </P>
  
</div>
  

  
      
<div  CLASS="slide">
  <h1>[back to implementation-level theory] capacity / robustness trade-off (Chaudhuri & Fiete, 2016)</h1>

  <img src="Chaudhuri16-fig5.png" class="figure-right" height=70%>
    <ol type=a>
      <li>
	If memory states, whether discrete or continuous (dark blue in
	left, right, respectively), are well-separated, a decoder can
	robustly recover the state from a relatively large amount of
	noise because the neighborhoods of each memory state are large
	(shaded light blue regions).
      </li>
      <li>
	Packing more memory states into the fixed state space volume
	of a given number of neurons necessarily means that the
	neighborhoods of each memory state will shrink. A small amount
	of noise drops the state into the neighborhood of a different
	memory state. Thus, higher capacity means less noise
	tolerance.
      </li>
    </ol>
    <HR>
      <P>
	<small>
	  Rishidev Chaudhuri and Ila Fiete
	  (2016). <a href="https://www.nature.com/articles/nn.4237"
		     target=new><I>Computational principles of memory</I></a>, Nature
	  Neuroscience 19(3):394.
	</small>
      </P>
  
</div>


      
<div  CLASS="slide">
  <h1>storing a continuous variable with discrete attractors (Chaudhuri & Fiete, 2016)</h1>

  <img src="Chaudhuri16-fig6.png" class="figure-right" height=70%>
    <ol type=a>
      <li>
	Values of a continuous variable (such as head orientation) can
	be naturally mapped onto a (quasi)continuous attractor of the
	same dimension and topology, preserving metric relationships
	between different values of the variable.
      </li>
      <li>
	To encode a continuous variable in a set of well-separated
	discrete stable states in some other coding dimension involves
	two steps: a discretization, followed by <B>THE HARDER STEP</B> of
	choosing how to map the discrete values into the
	attractors. In general, there is no metric-preserving mapping
	and the complexity of this encoding problem is high.
      </li>
    </ol>
    <HR>
      <P>
	<small>
	  Rishidev Chaudhuri and Ila Fiete
	  (2016). <a href="https://www.nature.com/articles/nn.4237"
		     target=new><I>Computational principles of memory</I></a>, Nature
	  Neuroscience 19(3):394.
	</small>
      </P>
  
</div>


      

<div  CLASS="slide">
  <h1>[computation and algorithm levels] ASSOCIATIVE MEMORY: balancing task requirements</h1>

  <a href="https://en.wikipedia.org/wiki/Columbarium"
     target=new><img src="columbarium.jpg"
		     class="figure-right"
		     title="a columbarium"
		     height=70%></a>
  <P>
    [THE HARDER STEP from the previous slide]
  </P>
  <P>
    How to get to the right neighborhood in the first place? <BR>
      And what to do if more than one item is retrieved?
  </P>
  <P>
    Answers to those questions will NOT be found on the implementation
    level (attractors dynamics) — we must consider the more abstract
    levels (behavioral / computational / algorithmic).
  </P>
  <P>
    Let's start with the TASK. For any task that involves
    storage/retrieval, the needs are:
      <ul>
	<li>high capacity (room for many items)</li>
	<li>convenient addressing (easy to figure out at what address a given
	  item is stored)</li>
	<li>quick access (not having to sift through too many items to find the
	  needed one)</li>
      </ul>
    </P>
      
</div>


  
<div  CLASS="slide">
  <img src="pandemonium.jpg" height=95% class="figure-right" >
  <h1>a historical example: Pandemonium</h1>

  <P>
    The requirements: a storage/retrieval mechanism with
    <ul>
      <li>high capacity (room for many items)</li>
      <li>convenient addressing (easy to figure out at what address a given
	item is stored)</li>
      <li>quick access (not having to sift through too many items to find the
	needed one)</li>
    </ul>
  </P>
  <P>
    One way of going about it: <a href="http://en.wikipedia.org/wiki/Oliver_Selfridge"
				  target=new>Selfridge</a>'s
    <a href="https://en.wikipedia.org/wiki/Pandemonium_architecture" 
       target=new>Pandemonium</a> model (1959). The illustration here
       is from
    <a href="https://books.google.com/books/about/Human_information_processing.html?id=SZsQAQAAIAAJ"
    target=new><I>Human Information Processing</I></a> (Lindsay & Norman, 1972)
  </P>
  <HR>
  <P class="incremental">
    A better, general-purpose algorithmic approach: HASHING.
  </P>

</div>


  

<DIV CLASS="slide">
  <h1>associative memory and hashing</h1>

  <img src="hashing.png" height=55% class="figure-right">
  <P>
    Hashing is a family of storage/retrieval algorithms that use
       a <a href="http://en.wikipedia.org/wiki/Hash_function"
	    target=new>hash function</a> to MAP CONTENT TO AN ADDRESS.
  </P>
  <P>
    [See
    this <a href="https://www.quantamagazine.org/scientists-find-optimal-balance-of-data-storage-and-time-20240208/"
    target=new>recent article</a> on hashing in the <I>Quanta</I> 
    magazine.]
  </P>
  <P>
    Hashing supports one-step (constant-time) access. But there are
    also downsides.
  </P>
  <P>
    Using hashing to implement associative memory leads to tension between
    two conflicting requirements:
    <ul>
      <li>
	on the one hand, we need fast recall with low probability of
	error ("collision", red arrows on the right);
      </li>
      <li>
	on the other hand, we need to preserve the similarity
	structure of the data (map similar items to nearby
	addresses). 
      </li>
    </ul>
  </P>
  <P>
    A way out through compromise: <B>locality-sensitive hashing</B>
    (think of the
    <a href="https://en.wikipedia.org/wiki/Dewey_Decimal_Classification"
       target=new>Dewey Decimal</a> library indexing)  —
  </P>
    <img src="general_vs_lsh.jpg" height=20%>
      
</div>



<DIV CLASS="slide">
  <h1>an illustration of locality-sensitive hashing (Kulis et al., IEEE TPAMI 2009)</h1>

  <img src="Kulis09-LSH-summary.png" height=55% class="figure-right">
  <P>
    Locality-sensitive hashing (LSH) combines one-step associative
    recall with <I>partial</I> preservation of the local similarity
    structure of the data.
  </P>
  <P>
    <I><B>Training</B> (top left of the diagram):</I> A list of \(k\) hash functions
    \(h_{r_1},\ldots,h_{r_k}\) (here, \(k=6\) for a six-bit address) is
    applied to <a href="https://en.wikipedia.org/wiki/Columbarium" target=new>pigeonhole</a> the \(N\)  items that need to be stored into the
    cells of a hash table, so that <font color=red>similar items</font> (here,
    similar-color) <font color=red>are likely to share a cell</font>.
  </P>
  <P>
    <I><B>Use</B> (bottom left):</I> To obtain the approximate near-neighbors for a query
    \(Q\), one need only hash it and then evaluate its similarity to the
    set of examples with which it "collides" (as opposed to the contents of the entire
    database, as in the Pandemonium).
  </P>
  <P>
    <I><B>Outcome</B> (bottom right):</I> The 3 out of 4 items in bin 110111 whose
    relevant characteristic (here, color) matches that of \(Q\).
  </P>

</div>



<div class="slide">
  <h1>hashing: post-processing after recall</h1>

  <table cellspacing="15">
    <tbody>
      <tr>
	<td rowspan="2" align="center">
	  <img src="pig-around-barn-droodle.jpg">
	  </td>
	<td>
	  <img src="dog-tail.jpg">
	  </td>
	<td rowspan=2 align="left">
	  Post-processing (further filtering) will typically be
	  required, but its complexity [number of elementary
	  operations needed to complete] is low, because most irrelevant
	  items have already been filtered out when the hash table was
	  populated with data.
	</td>
	</tr>
	  <tr>
	    <td>
	      <img src="pig-tail.jpg">
	      </td>
	    </tr>
	  </tbody>
	</table>
      
</div>




<DIV CLASS="slide">
  <h1>what kind of hashing function is needed for locality-sensitive hashing (LSH)?</h1>

  <img src="approximate-nearest-neighbor.jpg" height=50% class="figure-right">
  <P>
  ANY hashing function \(h\) that maps the data space to the memory
  address space is suitable for LSH, provided that the LSH condition is
  satisfied (Indyk & Motwani, 1998): 
  <DIR><DIR>
    <ul>
      <li>
      If two data points \({\bf p},{\bf q}\) are close to each other, the
      probability of mapping them to the same bin (memory cell) must be
      relatively large (below, larger than some \(P_1\)).
      </li>
      <li>
      If the two data points are not so close to each other, the probability
      of mapping them to the same bin must be relatively small (below,
      smaller than some small \(P_2\)).
      </li>
    </ul>
    $$
    \begin{alignat}{2}
    \textrm{if}~\|{\bf p} - {\bf q}\| \leq
    &~r~~~~&\textrm{then}~~~&~\textrm{P}\left[h({\bf p})=h({\bf q})\right]
    \geq P_1 \phantom{\ll P_1} ~~~(P_1~is~a~large~probability)\\
    \textrm{if}~\|{\bf p} - {\bf q}\| \geq
    &~cr~~~~&\textrm{then}~~~&~\textrm{P}\left[h({\bf p})=h({\bf q})\right]
    \leq P_2 \ll P_1 ~~~(P_2~is~a~small~probability)
    \end{alignat}
    $$
    </DIR></DIR>
    
</div>



<DIV CLASS="slide">
  <h1>locality-sensitive hashing (LSH) and the so-called "Chorus Transform" (Edelman & Shahbazi, 2012)</h1>

  <img src="Chorus-LSH.png" width=45% class="figure-right">
  <P>
    One of the LSH families described by Andoni and Indyk (2008) is 
    "multidimensional line partitioning":
  </P>
  <P>
    <ol>
      <li> perform a <B>random projection</B> of the data point
	\(\bf{p}\) onto a space that consists of <I>a few</I> lines — that
	is, \(\mathbb{R}^t\), where \(t\) grows slowly with \(n\), as for
	instance in \(t \sim \log n\); 
      </li>
      <li>
	<B>partition</B> the space \(\mathbb{R}^t\) into cells;
      </li>
      <li>
	make the hash function <B>return the index</B> of the cell that contains the
	projected point \(\bf{p}\).
      </li>
    </ol>
  </P>
  <HR width=50% align=left>
    <P>
      This is precisely what happens when the location of the data
      point relative to several LANDMARKS is computed, as illustrated
      here (see also the next slide for a reminder from
	<a href="https://shimon-edelman.github.io/Psych-3140/wk-5-1.html#(27)"
	   target=new>Lecture 5.1</a>).  A point projected onto the
	   gray plane activates four units TUNED to landmark
	   locations; the relative activation of each unit is depicted
	   by the size of the corresponding blue circle. The resulting
	   list of four numbers is the output of the hash function:
	   the memory address associated with the data point.
    </P>

</div>


<DIV CLASS="slide">
  <h1>recall from Lecture 5.1: using distances to landmarks</h1>

  <img src="Chorus-principle.png" height=45% class="figure-right">
    <P>
      An object can be effectively represented by the responses of
      several units that are broadly TUNED to LANDMARK shapes.
    </P>
    <P>
      A set of such tuned units are said to compute a <I>Chorus Transform</I> of the
      stimulus  \(\textbf{x}\)
      (here, <font face="courier">giraffe</font>), defined as the
      vector of distances (inverse similarities) between \(\textbf{x}\) and the landmarks
      \(\textbf{p}_i\)
      (here, <font 
	       face="courier">pig,
	camel,
	goat</font>): 
      $$
      CT({\bf x}) = \left( \begin{array}{c} 
      \| {\bf x} - {\bf p}_1 \| \\ \vdots \\ \| {\bf x} - {\bf p}_{n} \| 
      \end{array} \right)
      $$
    </P>
    <P>
      The application of the
      <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)"
      target=new>norm function</a> \(\|\cdot\|\) to each 
      difference vector \({\bf x} - {\bf p}_i\) yields its length and
      therefore the distance between \({\bf x}\) and \({\bf p}_i\).
    </P>
    <P>
      CT maps points from the original space with \(dim(\textbf{x}) = D\)
      dimensions to a space  spanned by distances to
      landmarks, which has \(n \ll D\) dimensions (\(n\) being the number of
      landmarks). In a typical situation in vision, this is some
      SERIOUS dimensionality reduction. 
    </P>

</div>

<!--

<DIV CLASS="slide">
  <h1>smooth dimensionality reduction with a Chorus of Prototypes</h1>

  <img src="chorus.jpg" height=350 class="figure-right">
  <P>
  The Chorus Transform of a point in a \(D\)-dimensional space \(\textbf{x}\in R^{D}\) — the vector of distances 
  between it and the landmarks \(\textbf{p}_i\) — performs dimensionality
  reduction from \(dim(\textbf{x}) = D\) to a potentially much smaller \(n \ll D\):
  $$
  CT({\bf x}) = \left( \begin{array}{c} 
  \| {\bf x} - {\bf p}_1 \| \\ \vdots \\ \| {\bf x} - {\bf p}_{n} \| 
  \end{array} \right)
  $$
  In other words, it maps points in a \(D\)-dimensional input space to
  points in a space whose dimensionality \(n\) is the same as the number of
  landmarks.
  <HR>
  <P>
    Note that \(CT\) is smooth in the requisite sense, and
    therefore the reduced-dimensionality representations it forms
    will be veridical (true) to those in the original space.
  </P>
  <P>
  <font color=gray>There is a deep and interesting link here to <a
  href="http://en.wikipedia.org/wiki/Kernel_method" 
  target=new>kernel methods</a> in machine learning, which are beyond the
  scope of this course.</font>

</div>

-->



<DIV CLASS="slide">
  <h1>reminder: why projection (as in LSH and in CT) can preserve veridicality</h1>

  <img src="projection.png" height=50% class="figure-right">
  <P>
    According to the <a
		       href="http://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma"
		       target=new>Johnson-Lindenstrauss Lemma</a> (1984), a cloud of points in a
    high-dimensional space can be projected onto a space of a VERY much lower
    (logarithmically lower) dimensionality, while largely preserving the
    relative distances among
    points.<sup><font color=red>*</font></sup>
  </P>
  <P>
    Relying on a related idea, Edelman (1999, App.B) showed that the
    Chorus Transform can support a logarithmic dimensionality
    reduction, while approximately preserving the relative distances
    among points (based on a theorem due
    to <a href="https://en.wikipedia.org/wiki/Jean_Bourgain"
    target=new>Jean Bourgain</a>, 1985).
  </P>
  <P>
    In other words, even with a very small number of landmarks — on
    the order of \(\log D\), where \(D\) is the dimensionality of the
    original problem — the layout of the data points in the new,
    low-dimensional space approximates their original layout, implying
    that the <B>original similarity</B> relations, and with them
    category boundaries, etc., are <B>largely preserved</B>.
  </P>
  <P>
    <HR width=30% align=left>
      <P>	
	<sup><font color=red>*</font></sup><font color=gray>Specifically, any \(n\)-point subset of Euclidean space can be
	  embedded in \(O(\epsilon^{−2} \log n)\) dimensions with at most \((1 +
	  \epsilon)\) distortion of the inter-point distances.</font>
      </P>

</div>



<DIV CLASS="slide">
  <h1>[BACK TO LSH] compare locality-sensitive hashing (LSH) and the CT (Edelman & Shahbazi, 2012)</h1>

  <img src="Chorus-LSH.png" width=45% class="figure-right">
  <font color=gray>
    <P>
      One of the LSH families described by Andoni and Indyk (2008) is 
      "multidimensional line partitioning":
    </P>
    <P>
      <ol>
	<li> perform a <B>random projection</B> of the data point
	  \(\bf{p}\) onto a space that consists of <I>a few</I> lines — that
	  is, \(\mathbb{R}^t\), where \(t\) grows slowly with \(n\), as in
	  \(t \sim \log n\); 
	</li>
	<li>
	  <B>partition</B> the space \(\mathbb{R}^t\) into cells;
	</li>
	<li>
	  make the hash function <B>return the index</B> of the cell that contains the
	  projected point \(\bf{p}\).
	</li>
      </ol>
    </P>
  </font>
  <HR width=50% align=left>
  <P>
    This is precisely what the Chorus Transform does:
    <ol>
      <li>Project \(\bf{p}\) onto the space
	(<a href="https://en.wikipedia.org/wiki/Manifold" target=new>manifold</a>)
	defined implicitly by the choice of landmarks/prototypes;</li>
      <li>Use the <a href="https://en.wikipedia.org/wiki/Receptive_field"
		     target=new>receptive field</a> of each tuned prototype unit 
	as a "feature detector" to compute part of the address 
	of the <a href="https://en.wikipedia.org/wiki/Columbarium"
		  target=new>"pigeonhole"</a> where the item will be stored. 
      </li>
    </ol>
  </P>

</div>

    
  

<DIV CLASS="slide">
  <h1>[EXTRA] CT and locality-sensitive hashing (Edelman & Shahbazi, 2012)</h1>

    <font color=gray>
      <P>
	To form a "digital" address,
	the outputs of the tuned units can be thresholded, in which
	case the address consists of the list of indices of units
	  whose activation by the probe point exceeds the threshold.
      </P>
      <P>
	Without thresholding, the Chorus Transform can be seen to carry out
	kernelized LSH (Kulis and Grauman, 2009). For instance, we may define
	the space \(V_j\) onto which the data are projected by the \(j\)-th hashing function
	as a linear combination of "landmarks" \(\left\{{\bf{z}}_n\right\}\) in the
	kernel space. This idea leads to the hash function
	$$
	\begin{eqnarray}
	h({\bf p}) = sign\left({\bf a}^{T} {\bf k}_{{\bf p}} - {\bf
	b}\right) 
	\end{eqnarray}
	$$
	where \(\bf{a}\) are the linear combination weights and
	$$
	\begin{eqnarray}
	{\bf k}_{\bf x}= \left[K\left({\bf x}, {\bf z}_{1}\right), \ldots,
	K\left({\bf x}, {{\bf z}}_{n}\right)\right]^{T}
	\end{eqnarray}
	$$
	are the kernel values between \(\bf{x}\) and each of the landmark points
	\({\bf{z}}_n\). With the distance function \(\|\cdot\|\) serving as the
	kernel and \({\bf{z}}_n\) as the prototypes, this is precisely the Chorus
	Transform of the data point \(\bf{x}\).
      </P>
    </font>

</div>



<DIV CLASS="slide">
  <h1>[CLIMATE] disaster memory and economic valuation</h1>

  <P>
    "The purchase and sale of assets such as housing will increasingly
    be affected by forces related to a <B>changing climate</B>. This
    article considers decisions over assets as a neurobiological
    process in which an <B>associative memory with pattern
    completion</B> informs choices.  We develop these neuroeconomic
    explanations and analyze their implications for climate
    change-related shocks in asset markets, and discuss these effects
    in the context of both individual experiences as well as
    community-driven remembering. These neuroeconomic models provide
    mechanistic explanations for behavioral responses to more easily
    accessed information (the “representativeness” and “availability”
    heuristics, “framing” and “priming”). Understanding the links from
    neuroscience to economics is critical to building policies and
    institutions capable of coping with and adjusting to disasters
    affecting real assets that are increasing in frequency and scope
    due to climate change."
  </P>
  <P>
    Runge, C. F., Johnson, J. A., Nelson, E., & Redish,
    A. D. (2023). <I>A neuroscience-based analysis of impacts of
    disaster memory on economic valuation</I>. Journal of
    Neuroscience, Psychology, and Economics, 16(1),
    24–49. <a href="https://doi.org/10.1037/npe0000168"
    target=new>https://doi.org/10.1037/</a>
  </P>
  
</div>  



<DIV CLASS="slide">
  <h1>[CLIMATE] disaster memory and economic valuation</h1>

  <img src="Redish23-fig1.png" class="figure-right" height=35%>
    <P>
      <I>Top:</I> The single-agent concept. We do not specifically
      model the world, the perceptual signals, nor the action in this
      model, but concentrate on the memory and valuation components.
    </P>
    <P>
      <I>Bottom:</I> Memory consists of a set of fully interconnected
      units forming a content-addressable memory that projects to a
      single valuation calculation. 
    </P>
    <P>
      Runge, C. F., Johnson, J. A., Nelson, E., & Redish,
      A. D. (2023). <I>A neuroscience-based analysis of impacts of
	disaster memory on economic valuation</I>. Journal of
      Neuroscience, Psychology, and Economics, 16(1),
      24–49. <a href="https://doi.org/10.1037/npe0000168"
		target=new>https://doi.org/10.1037/</a>
    </P>
  
</div>  



<DIV CLASS="slide">
  <h1>[CLIMATE] disaster memory and economic valuation</h1>

  <img src="Redish23-fig4.png" class="figure-right" height=55%>
  <P>
    We find that our simple model can explain the following observed
    effects in flood-related evaluation of housing values:
    <ol>
      <li>
	Experience with floods produces a decrease in asset valuation
	(i.e., an increase in assessed costs). These assessed costs
	decay with time, returning to zero through cognitive drift.
      </li>
      <li>
	Decay rates depend on the salience of the flood, while
	assessed cost depends on the cost of the flood.
      </li>
      <li>
	Reminders can reduce that rate of decay.
      </li>
      <li>
	Reassessments of the cost of a flood increase the expected
	cost, while alleviations can provide sudden decreases in
	expected cost.  However, neither of those changes in valuation
	affect the decay rate.
      </li>
  </P>
  <P>
    Runge, C. F., Johnson, J. A., Nelson, E., & Redish,
    A. D. (2023). <I>A neuroscience-based analysis of impacts of
    disaster memory on economic valuation</I>. Journal of
    Neuroscience, Psychology, and Economics, 16(1),
    24–49. <a href="https://doi.org/10.1037/npe0000168"
    target=new>https://doi.org/10.1037/</a>
  </P>
  
</div>  


<div class="footer">
<p>Last modified: Mon Feb 26 2024 at 15:13:28 EST</p>
</div>
</body>
</html>
