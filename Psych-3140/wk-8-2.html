<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>Psych 3140/6140 wk-8-2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<!-- <meta name="copyright" content="Copyright &169; 2010-2024 Shimon Edelman"/> -->
<meta name="font-size-adjustment" content="-1" /> <!-- DEFAULT SIZE -->
<link rel="stylesheet" href="../Slidy/w3c-blue3.css"
 type="text/css" media="screen, projection, print" />
 <link rel="stylesheet" href="extras.css"
 type="text/css" media="screen, projection, print" />
<script src="../Slidy/slidy.js" type="text/javascript">
</script>
<script type="text/javascript"
  src="../MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>

<!-- 
<rdf:RDF xmlns="http://creativecommons.org/ns#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<License rdf:about="http://creativecommons.org/licenses/by-sa/2.5/">
<permits rdf:resource="http://creativecommons.org/ns#Reproduction"/>
<permits rdf:resource="http://creativecommons.org/ns#Distribution"/>
<requires rdf:resource="http://creativecommons.org/ns#Notice"/>
<requires rdf:resource="http://creativecommons.org/ns#Attribution"/>
<permits rdf:resource="http://creativecommons.org/ns#DerivativeWorks"/>
<requires rdf:resource="http://creativecommons.org/ns#ShareAlike"/>
</License>
</rdf:RDF>
-->

<!-- this defines the slide background -->

<div class="background">
  <div class="header">
  <!-- sized and colored via CSS -->
  </div>
  <!-- hidden style graphics to ensure they are saved with other content -->
  <img class="hidden" src="../Slidy/bullet.png" alt="" />
  <img class="hidden" src="../Slidy/fold.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold.bmp" alt="" />
  <img class="hidden" src="../Slidy/fold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/nofold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/unfold-dim.bmp" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-fold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-nofold-dim.gif" alt="" />
  <img class="hidden" src="../Slidy/bullet-unfold-dim.gif" alt="" />

  <div class="footer">
  <!-- modify the following text as appropriate -->
  Week 8.2 &#151;
  </div>
</div>

<!-- COVER PAGE SLIDE -->
<div class="slide cover">
  <div class="header">
    <h1>Psych 3140/6140</h1>
    <p>Computing the Mind</p>
  </div>
  <div style="float:left">
    <h2>Week 8: Language</h2>
    <h3>&nbsp;Lecture 8.2: language learning (and use) </h3>
  </div>
  <img src="../Lake-Michigan-horizon.jpg" title="Computing the Mind"
  class="figure-right"  height=70%>

</div>
<!-- END COVER PAGE -->



<div  CLASS="slide">
  <h1>Lecture 8.2: language learning (and use)</h1>

  <img src="language-acquisition.gif" class="figure-right" >
  <P>
  Computational modeling of language acquisition (and use) —
  <ul>
    <li>Kinds of language structure and how to find them in a corpus</li>
    <P>
    <li>Patterns over patterns ("going recursive")</li>
    <P>
    <li>The role of <B>probabilities</B> in learning and using language</li>
    <P>
      <li>A telling, albeit modest, example: machine translation</li>
    <P>
      <li>Some challenges</li>
  </ul>
  
</div>




<DIV CLASS="slide">
  <h1>learning language structure, step 1: going digital (discrete)</h1>

  <P>
    <ul>
      <li>
	With discrete, categorical units, one can easily compute statistics
	&#151;  <SC>departures from equiprobability</SC> &#151;
	that make learning possible.
      </li>
      <li>
	Babies learn categorical representations of sound units, through
	exposure and socially guided learning, whose basis is <SC>alignment</SC>
	and <SC>comparison</SC> of snippets of speech. 
      </li>	
    </ul>
  </P>
  <P>
    For example, a <SC>contrastive</SC> pair, such as
    <DIR>  <DIR>
	<B>BIG</B>  and <B>BAG</B>
    </DIR>  </DIR>
  </P>
  <P>
    constitutes evidence that the content of the slot
    (<B>B<font color=red>__</font>G</B>) may serve as useful 
    unit.
  </P>

</div>




<DIV CLASS="slide">
  <h1>learning words from statistical regularities</h1>

  <img src="lang-prettybaby.gif" class="figure-right" >
  <P>
    One of the many relevant kinds of statistics is the <SC>transitional
      probability</SC> from one sound to next.
  </P>
  <P>
    It is highest when the two sounds follow one another within a <SC>word</SC>. 
    Transitional probabilities straddling a word boundary will be
    relatively low.
  </P>
  <P>
    For example, consider the sequence <B>prettybaby</B> in the context of a corpus in which
    <b>pretty</B> and <B>baby</b> appear paired with other words, and not just
    with each other: {<B>prettydoggie</B>, <B>prettybaby</B>,
    <B>nicedoggie</B>, <B>nicebaby</B>, <B>prettydoll</B>}.
  </P>
  <P>
    Now, the probability of
  <DIR><DIR>
      <B>pret</B>-><B>ty</B>
  </DIR></DIR>
  </P>
  <P>
    will be higher than the probability of
    <DIR><DIR>
	<B>ty</B>-><B>ba</B>
    </DIR></DIR>
  </P>

</div>




<DIV CLASS="slide">
  <h1>babies learning to segment words from statistical regularities</h1>

  <video src="Saffran-language.mp4" class="figure-right" controls> 
  </video>
  <P>
    The groundbreaking study of Saffran, Aslin, and Newport (1996) presented
    6-8 month old infants with 2-minute long snippets of synthesized
    speech in which the transitional probabilities were controlled.
  </P>
  <P>
    A habituation / head-turn procedure was used to quantify the ability of the subjects
    to segment speech into "words".
  </P>

  <HR>

    <P class="incremental">
      NOTE that habituation consists in the agent (here, the baby)
      learning the probabilities of certain transitions, thereby
      becoming capable of PREDICTING the upcoming item, given the
      preceding ones. This task — predicting the next item — is
      central to the training
      of <a href="https://en.wikipedia.org/wiki/Large_language_model"
      target=new>LLMs</a> such as <a href="https://chat.openai.com"
				     target=new>ChatGPT</a>.
    </P>

</div>




<DIV CLASS="slide">
  <h1>babies learning to segment words from statistical regularities</h1>

  <img src="lang-stat-Saffran96.jpg" height=450 class="figure-right" >
  <P>
    Average <SC>listening times</SC> (and standard errors) for 8-month-old infants
    familiarized with a <a href="Saffran-exsound.mp4" target=new>continuous sequence</a> of three-syllable items (novel
    "words") and then tested with two of these familiar items (shaded) and two
    items composed of parts of two words (open).
  </P>
  <P>
    <B>The infants treated part-words as novel items and thus distinguished
      them from whole words</B>.
  </P>

</div>


<!--

<DIV CLASS="slide">
  <h1>forward and backward probabilities</h1>

  <P>
  <BR>
  English:
  <DIR>
    go to school
  </DIR>
  <P>
  Korean: 
  <DIR>
    학교에 가자
  </DIR>

</DIV>



<DIV CLASS="slide">
  <h1>learning words from statistical regularities in a stream of (digital) information</h1>

  <img src="lang-MEX-alice2.jpg" height=400 class="figure-right">
  <P>
    Two stages in the processing of <a href="http://www.sabian.org/alice.htm"
				       target=new><I>Alice in Wonderland</I></a> by the <a
											  href="../SolanHornRuppinEdelman-PNAS05.pdf" target=new><SC>adios</SC></a>
    algorithm, more about which later.
  </P>
  <P>
    <small>
      Z. Solan,
      D. Horn, E. Ruppin and S. Edelman (2005). <I>Unsupervised Learning
	of Natural
	Languages</I>. Proc. Natl. Acad. Sci. 102:11629-11634.
    </small>
  </P>

</div>



<DIV CLASS="slide">
  <h1>given some structure, we can attempt generalization</h1>

  <P>
  <BR>
  сталинкаменеваубил
  <P>
  сталинзиновьеваубил
  <P>
  сталинфрунзеубил

  <P>
  <HR>
  <P>
  To find structure, <sc>align</SC> and <SC>compare</SC> substrings.

</div>




<DIV CLASS="slide">
  <h1>structure and generalization</h1>

  <P>
  <BR>
  сталинкаменеваубил
  <P>
  сталинзиновьеваубил
  <P>
  сталинфрунзеубил
  <P>
  сталинвсехчьялояльностьбылаподмалейшимподозрениемимногихчьялояльностьбылабезупречнойубил
  <P class="incremental">
  сталин<font color=red>всехчьялояльностьбылаподмалейшимподозрениемимногихчьялояльностьбылабезупречной</font>убил
  </P>
  <HR>
  <P>
  To find structure, <sc>align</SC> and <SC>compare</SC> substrings.
  <P>
  Then, feel free to substitute equivalent items [note the <a
 href="http://en.wikipedia.org/wiki/Accusative" target=new>accusative</a> <a
 href="http://en.wikipedia.org/wiki/Grammatical_case" target=new>case</a>] in the proper context.

</div>



<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: sentences of the first type</h1>

  <img src="Johnson65-list1.jpg">
  <P>
  <HR>
  <P>
  N. Johnson, <I>The psychological reality of phrase-structure rules</I>,
  J. of Verbal Learning and Verbal Behavior, 4:469-475 (1965).

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the hypothesized structure</h1>

  <img src="Johnson65-fig1a.jpg">

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: sentences of the second type</h1>

  <img src="Johnson65-list2.jpg">

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the hypothesized structure</h1>

  <img src="Johnson65-fig1b.jpg">  

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the results support the hypothesis</h1>

  <img src="Johnson65-fig1a.jpg">
  <img src="Johnson65-table2.jpg">
  <P>
  [The two-phrase results (first row in the transition error table) apply to this example]

</div>

<DIV CLASS="slide">
  <h1>psychological reality of phrase structure: the results support the hypothesis</h1>

  <img src="Johnson65-fig1b.jpg">
  <img src="Johnson65-table2.jpg">
  <P>
  [The three-phrase results (second row in the transition error table) apply to this example]

</div>


  -->



<DIV CLASS="slide">
  <h1>must not forget: the SOCIAL nature of language learning</h1>

  <img src="GoldsteinEtAl10-fig1.png" class="figure-right" height=550>
  <P>
    Language learning / use is an embodied, situated, incremental, dynamically
    constrained, concurrent, multimodal social behavior.
  </P>
  <HR>
  <P>
    <small>
      Michael H. Goldstein, Heidi R. Waterfall, Arnon Lotem, Joseph
    Y. Halpern, Jennifer A. Schwade, Luca Onnis, and Shimon Edelman
    (2010). <a href="https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(10)00045-8" target=new><I>General
    cognitive principles for learning structure in time and
	  space</I></a>, Trends in Cognitive Sciences 14:249-258.
    </small>
  </P>

</div>


  


<DIV CLASS="slide">
  <h1>one of the ways in which "Motherese" helps learning (here, alignment and comparison)</h1>

  <img src="varsets-in-six-languages.png" class="figure-right">
  <P>
    Runs of partially matching (= alignable) sentences occur naturally in
    child-directed speech.
  </P>
  <P>
    Naturalistic longitudinal studies (notably, by Heidi Waterfall) show that
    such <SC>variation sets</SC> facilitate mastery of structure by
    the child.
  </P>
  <P>
    The six examples here are all from the Child Language Data Exchange
    System, <a href="https://childes.talkbank.org/"
	       target=new>CHILDES</a>.
  </P>

  <HR>

    <P class="incremental">
      Language models such as
      <a href="https://chat.openai.com" target=new>ChatGPT</a> learn
      from "dead language" (text corpora), which lack almost all the
      rich information present in child-directed speech and in live
      social interactions that characterize child development. This is
      one of the reasons why LLMs require enormously more training
      data than babies are ever exposed to.
    </P>

  
</div>
  



<DIV CLASS="slide">
  <h1>variation sets in word segmentation: a controlled experiment</h1>

  <img src="Onnis-et-al-2008-fig2-cropped.png" class="figure-right">
  <P>
    Subjects listened to ~100 sentences generated by a small
    artificial grammar. Half of the subjects heard these sentences
    arranged so that ~20% of them formed variation sets; the other
    half heard <I>exactly the same sentences</I> in a randomly
    scrambled order (no variation sets).
    <PRE>
      ko si zu pa gu klo zi
      da pe ra pra ti
      ko si fa ma pju
      da zu pa pra ti
    </PRE>
  </P>
  <P>
    "Words" that appeared in variation sets were more reliably segmented.
  </P>
  <HR>
    <P>
      <small>
	Onnis, Waterfall, and Edelman
	(<a href="https://www.semanticscholar.org/paper/Learn-locally%2C-act-globally%3A-Learning-language-from-Onnis-Waterfall/da9605a78b7ca1b5092d9cb5ed794aacb60f3010" target=new><I>Learn
	    Locally, Act Globally: Learning
	    Language from Variation Set Cues</I></a>, Cognition 109:423-430,
	2008)
      </small>
    </P>

</div>
  

<!--

<DIV CLASS="slide">
  <h1>variation sets in phrase segmentation</h1>

  <P>
  <BR>
  <img src="Onnis-et-al-2008-fig4-cropped.png" class="figure-right">

  Can the effectiveness of variation sets be demonstrated in a controlled
  expriment?
  <P>
  The second experiment focused on phrase segmentation instead of word
  segmentation.
  <P class="incremental">
  The outcome was the same: <B>variation sets facilitated
  segmentation</B>.
  </P>

</div>

-->  


<DIV CLASS="slide">
  <h1>learning language structure, step 2: going recursive, finding PATTERNS</h1>

  <P>
    To learn <SC>hierarchical</SC> structure   ("complexity from simplicity"),
    one must <SC>"go recursive"</SC> &#151; find some statistically
    significant units, add them to the lexicon/grammar, use them to look for
    higher-order "units" (patterns or constructions), etc.
  </P>
  <P>
    Candidate structures found by <SC>aligning</SC> and <SC>comparing</SC> strings
    at a certain level of representation become units that participate
    in structure discovery at the next higher level.
  </P>
  <P>
  <HR>
  <table cellspacing=10 width=100%>
    <tr>
      <td align=center>
	the marmot in the hole
	<P>
	the big marmot
	<P>
	  the big brown marmot
	<P>
	  the marmot saw Trotsky
      </td>
      <td align=center>
	<img src="Minsky-marmot-Trotsky.jpg" height=250>
      </td>
    </tr>
    <!--
    <tr>
      <td align=center>
	<I>tail recursion</I>
      </td>
      <td align=center>
	<I>embedding recursion</I>
      </td>
    </tr>
    -->
  </table>

</div>



<DIV CLASS="slide">
  <h1>language structure: PATTERNS over words</h1>

  <img src="centaur+satyr-small.jpg" class="figure-right" height=550
  title="All I can say is thank goodness for teleconferencing.">

  <P>
  <a href="http://en.wikipedia.org/wiki/Collocation" target=new>Collocations</a>
  <P>
  <a href="http://en.wikipedia.org/wiki/Contrastive_distribution" target=new>Distributional equivalences</a>
  <P>
  Other dependencies (such as <a
  href="http://en.wikipedia.org/wiki/Agreement_%28linguistics%29"
  target=new>agreement</a>)
  <P>
  <HR>
  <P class="incremental">
    All these kinds of structure have the same mathematical form: they
    have to do with the CONDITIONAL PROBABILITY of an item/pattern
    appearing in an utterance, given that certain other items/patterns
    appear in that utterance, or elsewhere in the discourse, or in the
    situation in which it is embedded.
  </P>
  <!--
  <P class="incremental">
    As <a href="https://linguistlist.org/issues/3/3-445.html"
	  target=new>Zellig Harris</a> put it: "dependencies on
	  dependencies".
  </P>
  -->

</div>



<!--

<DIV CLASS="slide">
  <h1>structure from corpus: COLLOCATIONS</h1>

  <img src="lang-collocations-tab7-1.jpg" class="figure-right">

    <P>
      <BR>
    The pattern "What's all  this, then?" recurs suspiciously often (in
  particular contexts).

</div>




<DIV CLASS="slide">
  <h1>structure from corpus: DISTRIBUTIONAL EQUIVALENCES</h1>

  <img src="lang-complementary-distribution-tab7-2.jpg" class="figure-right">
  <P>
  <BR>
  "tobacconist's" and "record" can be interchanged in the context of
  <P> "I will not buy this ___, it is scratched"
  <P>
    <video src="Monty Python's Flying Circus - Dirty Hungarian Phrasebook.mp4" 
	   controls="controls">
    </video>
	   
</div>




<DIV CLASS="slide">
  <h1>structure from corpus: AGREEMENT</h1>

  <img src="lang-agreement.png" class="figure-right">

  <P>
  <BR>
  The choice of an item for one of the slots in a structure affects the
  choice of items for the other slots:
  <P style="font-family : courier;">
  <B>[I ] sleep[_]
  <BR>[he] sleep[s]</B>
  </P>
  <P style="font-family : courier;">
  <B>[I ] work[_]
  <BR>
  [he] work[s]</B>
  </P>
  <video src="Monty-Python-Lumberjack-song-dependency.mp4"
	 controls="controls"
	 </video>
  
</div>



<DIV CLASS="slide">
  <h1>a corpus of utterances has the form of a GRAPH, which can be
  used for alignment and comparison</h1>

  <img src="ADIOS-graph.gif" class="figure-right" height=450>
  <P>
  <BR>
  <SC>Collocations</SC>, <SC>equivalences</SC>, and other
  <SC>dependencies</SC> can be discovered by <SC>aligning</SC> and 
  <SC>comparing</SC> paths through the corpus <a href="http://en.wikipedia.org/wiki/Graph_%28mathematics%29"
  target=new><SC>graph</SC></a>. 

</div>

-->  


  
<DIV CLASS="slide">
  <h1>patterns over patterns ("going recursive")</h1>

  <img src="lang-fig7-9.gif" class="figure-right"  height=500>
  <P>
      <SC>Collocations</SC>, <SC>equivalences</SC>, and other
      <SC>dependencies</SC> can be discovered by <SC>aligning</SC> and 
      <SC>comparing</SC> paths through the corpus
      <a href="http://en.wikipedia.org/wiki/Graph_%28mathematics%29"
	 target=new><SC>graph</SC></a>. 
  </P>
  <P>
    The examples on the right illustrate recursive application of
    structure discovery by the ADIOS algorithm to a corpus of data
    yields hierarchical tree-like structures — patterns
    or <B>constructions</B>, which combine form and function, and can
    be used generatively to meet functional needs.
  </P>
  <P>
    <small>Z. Solan, D. Horn, E. Ruppin and S. Edelman, (2005).
      <a href="../SolanHornRuppinEdelman-PNAS05.pdf"
	 target=new><I>Unsupervised Learning of Natural Languages</I></a>,  
      Proc. Natl. Acad. Sci. 102:11629-11634.
    </small>
  </P> 

  <!--

  <P>
  <HR>
  <P>
  Whether or not such structures are psychologically real is a separate
  question (which is part of the subject matter of <a
  href="http://en.wikipedia.org/wiki/Psycholinguistics"
  target=new>psycholinguistics</a>).

  -->

</div>


<DIV CLASS="slide">
  <h1>patterns/<SC>constructions</SC> are <B>generative</B>; they FORM
  meaningful TOOLS</h1>
  
  <img src="lang-fig7-11.gif" height=375 class="figure-right">
  <P>
  A CONSTRUCTION learned by the ADIOS algorithm from
  the <a href="https://childes.talkbank.org/" target=new>CHILDES</a>
  corpus, which generates such sentences as:
  <P>
    <B>where's the big room?</B>
    <P>
    <B>where's the yellow one?</B>
    <P>
    <B>where's Becky?</B>
    <P>
    <B>where's that?</B>
  
</div>


<DIV CLASS="slide">
  <h1>this kind of learning works not just in English</h1>

  <img src="Bellagio.025.png" class="figure-right" height=500>
    <P>
      Half of these sentences are from one of the Mandarin corpora
      in <a href="https://childes.talkbank.org/"
      target=new>CHILDES</a>. The other half were generated by the
      ADIOS algorithm after learning from these corpora. [All these
      results are from ~20 years ago.] 
    </P>

</div>





<DIV CLASS="slide">
  <h1>populating the open slots in the patterns: probabilities at work</h1>

  <img src="ADIOS-SLM.gif" class="figure-right" >
  <P>
    Conditional probabilities on constructions: <SC>selection</SC> ("food"
    vs. "victuals")
  </P>
  <P>
    Conditional probabilities on constructions: <SC>dependencies</SC>
    ("served" vs. "serving")
  </P>
  <P>
    <I>Right:</I> The structural/statistical
    <a href="https://en.wikipedia.org/wiki/Language_model"
       target=new>language model</a> learned by the ADIOS algorithm
    captures those probabilities and puts them to use.
  </P>

</div>





<DIV CLASS="slide">
  <h1>populating the open slots in the patterns: probabilities at work</h1>

  <img src="fakeSmithBarney.gif" height=450 class="figure-right">
    <P>
      Getting these probabilities wrong is often the downfall of
      Internet scams (especially if the scammers have learned the
      language from a grammar-heavy textbook).
    </P>
    <P class="incremental">
      "obligatory to follow" WTF?
    </P>

</div>




<DIV CLASS="slide">
  <h1>the probability of "obligatory to follow"</h1>
  
  <img src="obligatory-to-follow.jpg" class="figure-right" height=550>
    <P>
      745 Google hits for "obligatory to follow"
    </P>

</div>



  <DIV CLASS="slide">
    <h1>the probability of "must be followed"</h1>

    <img src="must-be-followed.jpg" class="figure-right" height=550>
      <P>
	5,650,000 Google hits for "must be followed"
      </P>
      <P class="incremental">
	...but even this sounds off in the context of a letter
	from a bank... (a propos
	<a href="https://en.wikipedia.org/wiki/Pragmatics"
	   target=new>pragmatics</a>, <a href="https://en.wikipedia.org/wiki/Register_(sociolinguistics)"
					 target=new>register</a>, etc.) 
      </P>

</div>


<DIV CLASS="slide">
 <h1>the probabilistic language model in your phone and its relatives</h1>

 <a href="https://xkcd.com/1427/" target=new><img src="xkcd-ios-keyboard-language-model.png" class="figure-right"></a>
    <P>
      [Cf. <a href="http://languagelog.ldc.upenn.edu/nll/?p=3677"
	      target=new>Mad Libguistics</a>]
    </P>
   <P>
     <a href="https://openai.com/blog/better-language-models/"
	target=new>GPT-2</a> (Generative Pre-trained Transformer 2, 2019)
   </P>
   <P>
     <a href="https://gptcrush.com/resources/" target=new>GPT-3</a>
     (2021; see
     <a href="https://lambdalabs.com/blog/demystifying-gpt-3/"
     target=new>here</a> for some background
     and <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html"
     target=new>here</a> for an overview of its architecture)
   </P>
   <P>
     <a href="https://openai.com/research/gpt-4" target=new>GPT-4</a> (March 2023)
   </P>
    
</div>

<!--

<DIV CLASS="slide">
  <h1>how do we model all this mess: (part of) the big picture</h1>

  <img src="taxonomy-of-models.png" height=450 class="figure-right">
  <P>
    A taxonomy of computational models of language acquisition (and use) —
  </P>
  <P>
    <small>
      [S. Edelman
      (2017). <a href="http://doi.org/10.1016/j.langsci.2017.04.003"
		 target=new><I>Language and other complex behaviors: 
	  unifying characteristics, computational models, neural mechanisms</I></a>,
      Language Sciences 62:91-123.]
    </small>
  </P>
  <HR>
  <P>
    What about the GPT-3 innards? See
    <a href="https://dugas.ch/artificial_curiosity/GPT_architecture.html"
       target=new>here</a>.
  </P>
  <HR>
  <P class="incremental">
    CONSTRUCTIONS — snippets of language that combine FORM AND
    FUNCTION — belong on the right in this diagram. <BR> <BR> How does
    Construction Grammar fare as a theory of human language?
  </P>
  
</div>



<DIV CLASS="slide">
  <h1>recap: the structure of language is like...</h1>

  <a href="http://www.tfl.gov.uk/tfl/tube_map.shtml" target=new><img src="tube_map.gif" height=500></a>

</div>




<DIV CLASS="slide">
  <h1>recap: the structure of language is like a subway system</h1>

  <font size=+3>
  <table cellspacing=10>
    <tr><td><SC>lexicon-grammar</SC></td><td>&#8596;</td><td>the entire transit system</td></tr>
    <tr><td><SC>grammatical utterances</SC></td><td>&#8596;</td><td>possible trips</td></tr>
    <tr><td><SC>learning</SC></td><td>&#8596;</td><td>constructing the transit system</td></tr>
    <tr><td><SC>probabilistic selection</SC></td><td>&#8596;</td><td>traffic flow</td></tr>
    <tr><td><SC>structure vs. statistics</SC></td><td>&#8596;</td><td>routes vs. traffic</td></tr>
    <tr><td><SC>fluency and disfluency</SC></td><td>&#8596;</td><td>navigation proficiency</td></tr>
    <tr><td><SC>meaning</SC></td><td>&#8596;</td><td>the <strike>&nbsp;outside&nbsp;</strike> world</td></tr>
  </table>
  </font>

</div>

-->



<DIV CLASS="slide">
  <h1>some structural challenges, and how a CONSTRUCTION-based approach can meet them</h1>

  <img src="Goldberg16-tab1.png" class="figure-right" height=350>
    <P>
      CHALLENGE: anaphoric <I>one</I> has the same grammatical
      distribution as numeral <I>one</I>; how can the difference be
      learned?
    </P>
    <P>
      <small>
	The only formal distinction between anaphoric <I>one</I> and
	the elliptical use of numeral <I>one</I> is that
	numeral <I>one</I> receives a sentence accent, as indicated by
	capital letters in Table 1, whereas anaphoric <I>one</I> must
	be unstressed. [...]  The difference in accent between
	cardinal and anaphoric <I>one</I> reflects a key difference in
	their <B>functions</B>. Whereas cardinal <I>one</I> is used to
	assert the quantity “1,” anaphoric <I>one</I> is used when
	quality or existence — not quantity — is at issue.
      </small>
    </P>
    <P>
      <ul>
	<li>If we are too quick to assume a purely syntactic
	  generalization, it is easy to be led astray.</li>
	<li>It is important to recognize
	  relationships among constructions. In particular, anaphoric <I>one</I>
	  is systematically related to numeral <I>one</I>, and a comparison of
	  the FUNCTIONAL properties of these closely related forms
	  serves to explain their distributional properties.</li>
      </ul>
    </P>
    <HR>
  <P>
    <small>
      Adele E. Goldberg, <I>Subtle implicit language facts emerge from
	the functions of constructions</I>, Frontiers in Psychology,
	6:2019 (2016).
    </small>
  </P>
  
</div>


<DIV CLASS="slide">
  <h1>some structural challenges, and how a CONSTRUCTION-based approach can meet them (cont.)</h1>

  <img src="Goldberg16-tab2.png" class="figure-right" height=350>
    <P>
      CHALLENGE: explain the many peculiarities of long-distance
      dependencies (such as the one illustrated on the next slide). 
    </P>
    <P>
      Informally, long-distance dependencies (LDDs) arise when some
      part of the sentence is brought into
      <a href="https://en.wikipedia.org/wiki/Focus_(linguistics)"
      target=new>FOCUS</a>.
    </P>
    <P>
      For instance, in a wh-question, the wh-word is brought to the
      front of the sentence — possibly far from where it would
      "normally" appear.
    </P>
    <HR>
  <P>
    <small>
      Adele E. Goldberg, <I>Subtle implicit language facts emerge from
	the functions of constructions</I>, Frontiers in Psychology,
	6:2019 (2016).
    </small>
  </P>
  
</div>


<DIV CLASS="slide">
  <h1>some structural challenges, and how a CONSTRUCTION-based approach can meet them (cont.)</h1>

  <img src="Goldberg16-tab3.png" class="figure-right" height=450>
    <P>
      CHALLENGE: explain the marginal acceptability of the
      interrogative sentences in the examples on the right (marked by
      ??).
    </P>
    <P>
      Explanation: it follows from a clash of the functions of LDD
      constructions and
      <a href="https://en.wikipedia.org/wiki/Wh-movement#Extraction_islands"
      target=new>island constructions</a>. A referent cannot
      felicitously be both discourse-prominent (in the LDD
      construction, when used for FOCUS) and backgrounded in discourse
      (in the island construction, when used for a "side remark").
    </P>
    <HR>
  <P>
    <small>
      Adele E. Goldberg, <I>Subtle implicit language facts emerge from
	the functions of constructions</I>, Frontiers in Psychology,
	6:2019 (2016).
    </small>
  </P>
  
</div>


<DIV CLASS="slide">
  <h1>[EXTRA] some structural challenges, and how a CONSTRUCTION-based
  approach can meet them (cont.)</h1>

  <img src="Goldberg16-tab4.png" class="figure-right" height=450>
    <P>
      <I>Right:</I> The functional account predicts the pattern behind
      certain cases, even though they are exceptional from a purely
      syntactic point of view. (See the paper for an explanation!)
    </P>
    <HR>
  <P>
    <small>
      Adele E. Goldberg, <I>Subtle implicit language facts emerge from
	the functions of constructions</I>, Frontiers in Psychology,
	6:2019 (2016).
    </small>
  </P>
  
</div>




<DIV CLASS="slide">
  <h1>moving on: language comprehension and production</h1>

  <img src="EdelmanSolan09-fig1.png" class="figure-right" height=500>
    <P>
      A working model of machine translation, which at the time (in
      2009) outperformed Google Translate.
    </P>
    <P>
      <small>S. Edelman and
	Z. Solan
	(2009). <a href="../Edelman-Solan-PACLIC09.pdf" target=new><I>Machine
	Translation Using Automatically
	Inferred Construction-based Correspondence and Language
	Models</I></a>, Proc. 23rd Pacific Asia Conference on Language,
	Information, and Computation (PACLIC-23), Hong Kong, December
	2009.</small>
    </P>
    <P class="incremental">
      To replicate the dynamics of dialogic language use, the same
      approach can be combined with
      <a href="https://pdfs.semanticscholar.org/0036/cd484a3a9517653aecf13a1d14201b32c18f.pdf"
	 target=new>competitive queuing</a>.
      <img src="Bohland10-competitive-queuing.jpg" height=200>
      </P>

</div>    

    


<DIV CLASS="slide">
  <h1>ChatGPT on Chomsky (from S. Piantadosi, 2023)</h1>

  <P>
    "Large language models rewrite the philosophy of approaches to
    language.  It’s worth ending with a reminder of how impressive
    these models are:
  </P>
  <img src="Piantadosi23-1.png" height=90%>

</div>



<DIV CLASS="slide">
  <h1>ChatGPT on Chomsky (from S. Piantadosi, 2023) (cont. from
  previous slide)</h1>

  <img src="Piantadosi23-2.png" width=70%>

</div>

  
  
<DIV CLASS="slide">
  <h1>a reality check: AI-by-learning (from sampled behaviors)</h1>

  \(\require{amsmath}\)
  \(\require{amssymb}\)

  <img src="vanRooij23-fig2.png" class="figure-right" height=65%>
  <P>
    A visual illustration of the hypothetical learning scenario and
    its formalisation: Dr. Ingenia has access (magically and at no
    cost) to any machine learning method \(\mathhbb{M}\), present or
    future, and by repeatedly sampling data \(D\) from the
    distribution \(\cal{D}\) they can use whatever \(\mathhbb{M}\)
    they like to create a program \(L_{A}\) that when implemented and
    run generates behaviours \(b = A(s)\) when prompted by different
    situations \(s\).  The goal is to generate with non-negligible
    probability \(\delta(n)\) an algorithm \(A\) that behaves
    (approximately) human-like, in the sense that \(A\) is
    non-negligibly (\(\epsilon(n)\)) better than chance at
    picking behaviours that are possible for \(s\) in
    \(\cal{D}\). Here, \(n\) is a measure of the situation complexity,
    i.e., the maximum length of strings (\(\vert s\vert\)) needed to
    encode the relevant information in the situations \(s\).
  </P>
  <P>
    <small>
      Iris van Rooij, Olivia Guest, et
      al. (2023). <a href="https://osf.io/preprints/psyarxiv/4cbuv"
      target=new><I>Reclaiming AI as a theoretical tool for cognitive
      science</I></a>. Preprint.
    </small>
  </P>
  
</div>



<DIV CLASS="slide">
  <h1>a reality check: AI-by-learning</h1>

  <P>
    <SC>AI-by-Learning</SC> (informal)
  </P>
  <P>
    <I>Given:</I> A way of sampling from a distribution \(\cal{D}\).
  </P>
  <P>
    <I>Task:</I> Find an algorithm \(A\) (i.e., ‘an AI’) that, when
    run for different possible situations as input, outputs behaviours
    that are human-<I>like</I> (i.e., approximately like \(\cal{D}\)
    for some meaning of ‘approximate’).
  </P>
  <P>
    <B>Theorem 2</B> (Ingenia Theorem). <SC>AI-by-Learning</SC> is intractable.
  </P>
  <P>
    <small>
      Iris van Rooij, Olivia Guest, et
      al. (2023). <a href="https://osf.io/preprints/psyarxiv/4cbuv"
      target=new><I>Reclaiming AI as a theoretical tool for cognitive
      science</I></a>. Preprint.
    </small>
  </P>

</div>

  

<div class="footer">
<p>Last modified: Tue Mar 12 2024 at 16:16:40 EDT</p>
</div>
</body>
</html>
